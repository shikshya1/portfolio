<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>NLP on Shikshya Dahal</title><link>https://shikshya1.github.io/portfolio/nlp/</link><description>Recent content in NLP on Shikshya Dahal</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 02 Mar 2017 12:00:00 -0500</lastBuildDate><atom:link href="https://shikshya1.github.io/portfolio/nlp/index.xml" rel="self" type="application/rss+xml"/><item><title>Measuring Distance</title><link>https://shikshya1.github.io/portfolio/nlp/project-6/</link><pubDate>Fri, 07 Oct 2022 10:58:08 -0400</pubDate><guid>https://shikshya1.github.io/portfolio/nlp/project-6/</guid><description>Eucledian Distance Eucledian distance measures the distance between two points.
Cosine similarity Cosine similarity measures how similar the documents are irrespective of their size. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. It is a judgment of orientation and not magnitude. The cosine similarity is advantageous because even if the two similar documents are far apart by the Euclidean distance (due to the size of the document), chances are they may still be oriented closer together.</description></item><item><title>Text Representation</title><link>https://shikshya1.github.io/portfolio/nlp/project-5/</link><pubDate>Thu, 06 Oct 2022 10:58:08 -0400</pubDate><guid>https://shikshya1.github.io/portfolio/nlp/project-5/</guid><description>Text Representation Maps each word in the vocabulary(V) of the text corpus to a unique ID (integer value), then represent each sentence or document in the corpus as a V-dimensional vector.
One-Hot encoding: In one-hot encoding, each word w in the corpus vocabulary is given a unique integer ID w id that is between 1 and |V|, where V is the set of the corpus vocabulary. Each word is then represented by a V-dimensional binary vector of 0s and 1s.</description></item><item><title>NER</title><link>https://shikshya1.github.io/portfolio/nlp/project-4/</link><pubDate>Wed, 05 Oct 2022 10:58:08 -0400</pubDate><guid>https://shikshya1.github.io/portfolio/nlp/project-4/</guid><description>Named Entity Recognition NER refers to the IE task of identifying the entities in a document. Entities are typically names of persons, locations, and organizations, and other specialized strings, such as money expressions, dates, products, names/numbers of laws or articles, and so on. NER is an important step in the pipeline of several NLP applications involving information extraction.
A simple approach to building an NER system is to maintain a large collection of person/organization/location names that are the most relevant to our company (e.</description></item><item><title>Text Classification</title><link>https://shikshya1.github.io/portfolio/nlp/project-3/</link><pubDate>Tue, 04 Oct 2022 10:58:08 -0400</pubDate><guid>https://shikshya1.github.io/portfolio/nlp/project-3/</guid><description>Text Classification Classification categorizes data into one or more known classes. The data can be text, speech, image or numeric. Classification is used in wide range of applications across multiple domains such as: social media, healthcare, e-commerce etc. In today&amp;rsquo;s post, I will be exploring text classification.
This notebook linked with this post demostrates how to use several ML algorigthms for the disaster classification task. The models built for this purpose in this notebook are:</description></item><item><title>Interpreting Classification Models</title><link>https://shikshya1.github.io/portfolio/nlp/project-2/</link><pubDate>Sun, 02 Oct 2022 10:58:08 -0400</pubDate><guid>https://shikshya1.github.io/portfolio/nlp/project-2/</guid><description>Interpreting Classification Models When we are building classification models; we are most of the time concerned about the performance criteria (how accurate the model is to classify the output labels). We donot look for explanations as to why it decided to classify the input to a certain class. But there are situations where questioning the decision of classification model becomes necessary.
Suppose we are building classifier that identifies spam emails. It identifies certain email as &amp;lsquo;spam&amp;rsquo; and moves it to spam folder rather than placing it in inbox folder without the need of human intervention.</description></item><item><title>NLP 1: NLP pipelines</title><link>https://shikshya1.github.io/portfolio/nlp/project-1/</link><pubDate>Tue, 07 Apr 2020 10:58:08 -0400</pubDate><guid>https://shikshya1.github.io/portfolio/nlp/project-1/</guid><description>NLP :
&amp;ldquo;As we all know, dealing with natural language is hard. It is hard from the standpoint of the child, who must spend many years acquiring a language (compare this time span to that required for the acquisition of motor skills such as eating solids, walking, or swimming), it is hard for the adult language learner, it is hard for the scientist who attempts to model the relevant phenomena, and it is hard for the engineer who attempts to build systems that deal with natural language input or output.</description></item></channel></rss>