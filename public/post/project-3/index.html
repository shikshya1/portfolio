<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>NLP 2: Text Representation | Shikshya Dahal</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="Pierre Gringoire">
    <meta name="generator" content="Hugo 0.68.3" />
    
    
    
    
      <meta name="robots" content="noindex, nofollow">
    

    
<link rel="stylesheet" href="/shikshya/ananke/css/main.min.css" >



    
    
    
      

    

    
    
    <meta property="og:title" content="NLP 2: Text Representation" />
<meta property="og:description" content="Pierre Gringoire" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://shikshya-d.github.io/shikshya/post/project-3/" />
<meta property="article:published_time" content="2017-04-10T11:00:59-04:00" />
<meta property="article:modified_time" content="2017-04-10T11:00:59-04:00" /><meta property="og:site_name" content="Shikshya Dahal" />
<meta itemprop="name" content="NLP 2: Text Representation">
<meta itemprop="description" content="Pierre Gringoire">
<meta itemprop="datePublished" content="2017-04-10T11:00:59-04:00" />
<meta itemprop="dateModified" content="2017-04-10T11:00:59-04:00" />
<meta itemprop="wordCount" content="1650">



<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="NLP 2: Text Representation"/>
<meta name="twitter:description" content="Pierre Gringoire"/>

	
  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="/shikshya/" class="f3 fw2 hover-white no-underline white-90 dib">
      
        Shikshya Dahal
      
    </a>
    <div class="flex-l items-center">
      

      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/shikshya/about/" title="About page">
              About
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/shikshya/contact/" title="Contact page">
              Contact
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/shikshya/post/" title="Projects page">
              Projects
            </a>
          </li>
          
        </ul>
      
      
<div class="ananke-socials">
  
    <a href="https://github.com/shikshya1" target="_blank" class="github ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="GitHub link" rel="noopener" aria-label="follow on GitHub——Opens in a new window">
      
        <span class="icon"><svg style="enable-background:new 0 0 512 512;" version="1.1" viewBox="0 0 512 512"  xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
  <path d="M256,32C132.3,32,32,134.8,32,261.7c0,101.5,64.2,187.5,153.2,217.9c11.2,2.1,15.3-5,15.3-11.1   c0-5.5-0.2-19.9-0.3-39.1c-62.3,13.9-75.5-30.8-75.5-30.8c-10.2-26.5-24.9-33.6-24.9-33.6c-20.3-14.3,1.5-14,1.5-14   c22.5,1.6,34.3,23.7,34.3,23.7c20,35.1,52.4,25,65.2,19.1c2-14.8,7.8-25,14.2-30.7c-49.7-5.8-102-25.5-102-113.5   c0-25.1,8.7-45.6,23-61.6c-2.3-5.8-10-29.2,2.2-60.8c0,0,18.8-6.2,61.6,23.5c17.9-5.1,37-7.6,56.1-7.7c19,0.1,38.2,2.6,56.1,7.7   c42.8-29.7,61.5-23.5,61.5-23.5c12.2,31.6,4.5,55,2.2,60.8c14.3,16.1,23,36.6,23,61.6c0,88.2-52.4,107.6-102.3,113.3   c8,7.1,15.2,21.1,15.2,42.5c0,30.7-0.3,55.5-0.3,63c0,6.1,4,13.3,15.4,11C415.9,449.1,480,363.1,480,261.7   C480,134.8,379.7,32,256,32z"/>
</svg>
</span>
      
<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000"  xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;"/>
</svg>
</span></a>
  
</div>
    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked">
          
        PROJECTS
      </aside>
      










  <div id="sharing" class="mt3 ananke-socials">
    
  </div>


      <h1 class="f1 athelas mt3 mb1">NLP 2: Text Representation</h1>
      
      
      
      <time class="f6 mv4 dib tracked" datetime="2017-04-10T11:00:59-04:00">April 10, 2017</time>
      

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><p>Text representation deals with reptresenting text mathematically. These approaches are divided into four categories:</p>
<ol>
<li>Basic vectorization approaches</li>
<li>Distributed representations</li>
<li>Universal language representations</li>
<li>Handcrafted representations</li>
</ol>
<p>Vector space models:
Text data must be converted into some mathematical form in order to work with ML algorithms.Representing text units (characters, phonemes, words,
phrases, sentences, paragraphs, and documents) with vectors of numbers is
known as the vector space model (VSM). The most common way to calculate similarity between two
text blobs is using cosine similarity: the cosine of the angle between their
corresponding vectors. The cosine of 0° is 1 and the cosine of 180° is –1, with the
cosine monotonically decreasing from 0° to 180°.</p>
<p>Text representation schemes is differentiated on the basis of how well the
resulting vector captures the linguistic properties of the text it represents.</p>
<ol>
<li>Basic vectorization approaches :</li>
</ol>
<p>map each word in the vocabulary(V) of the text corpus to a unique ID (integer value), then represent each sentence or document in the corpus as a V-dimensional vector.</p>
<ul>
<li>
<p>One-Hot encoding :
In one-hot encoding, each word w in the corpus vocabulary is given a unique integer
ID w id that is between 1 and |V|, where V is the set of the corpus vocabulary. Each
word is then represented by a V-dimensional binary vector of 0s and 1s.</p>
</li>
<li>
<p>Bag of Words :
Represent the text under consideration as a bag (collec‐
tion) of words while ignoring the order and context.</p>
</li>
</ul>
<p>Advantages:</p>
<p>Simple to understand and implement.
With this representation, documents having the same words will have their vec‐
tor representations closer to each other in Euclidean space as compared to docu‐
ments with completely different words.</p>
<p>Disadvantages:</p>
<p>• The size of the vector increases with the size of the vocabulary. Thus, sparsity
continues to be a problem. One way to control it is by limiting the vocabulary to
n number of the most frequent words.
• It does not capture the similarity between different words that mean the same
thing. Say we have three documents: “I run”, “I ran”, and “I ate”. BoW vectors of
all three documents will be equally apart.
• This representation does not have any way to handle out of vocabulary words
(i.e., new words that were not seen in the corpus that was used to build the vec‐
torizer).
• As the name indicates, it is a “bag” of words—word order information is lost in
this representation. Both D1 and D2 will have the same representation in this
scheme.</p>
<ul>
<li>Bag of N-Grams:</li>
</ul>
<p>Breaks text into chunks of n contiguous words (or
tokens). This can help us capture some context, which earlier approaches could not
do. Each chunk is called an n-gram. The corpus vocabulary, V, is then nothing but a
collection of all unique n-grams across the text corpus. Then, each document in the
corpus is represented by a vector of length |V|. This vector simply contains the fre‐
quency counts of n-grams present in the document and zero for the n-grams that are
not present.</p>
<p>To elaborate, let’s consider our example corpus.</p>
<p>documents = [&ldquo;Dog bites man.&quot;, &ldquo;Man bites dog.&quot;, &ldquo;Dog eats meat.&quot;, &ldquo;Man eats food.&quot;]</p>
<p>Let’s construct a 2-gram (a.k.a.
bigram) model for it. The set of all bigrams in the corpus is as follows: {dog bites,
bites man, man bites, bites dog, dog eats, eats meat, man eats, eats food}. Then, BoN
representation consists of an eight-dimensional vector for each document. The
bigram representation for the first two documents is as follows: D 1 : [1,1,0,0,0,0,0,0],
D 2 : [0,0,1,1,0,0,0,0]. The other two documents follow similarly. Note that the BoW
scheme is a special case of the BoN scheme, with n=1. n=2 is called a “bigram model,”
and n=3 is called a “trigram model.” Further, note that, by increasing the value of n,
we can incorporate larger context; however, this further increases the sparsity. In NLP
parlance, the BoN scheme is also called “n-gram feature selection.</p>
<p>Pros and Cons:</p>
<p>• It captures some context and word-order information in the form of n-grams.
• Thus, resulting vector space is able to capture some semantic similarity. Docu‐
ments having the same n-grams will have their vectors closer to each other in
Euclidean space as compared to documents with completely different n-grams.
• As n increases, dimensionality (and therefore sparsity) only increases rapidly.
• It still provides no way to address the OOV problem.</p>
<ul>
<li>TF-IDF</li>
</ul>
<p>The intuition behind TF-IDF is as follows: if a word w appears many times in a docu‐
ment d i but does not occur much in the rest of the documents d j in the corpus, then
the word w must be of great importance to the document d i . The importance of w
should increase in proportion to its frequency in d i , but at the same time, its impor‐
tance should decrease in proportion to the word’s frequency in other documents d j in
the corpus. Mathematically, this is captured using two quantities: TF and IDF. The
two are then combined to arrive at the TF-IDF score.</p>
<p>Disadvantages of these approaches are:</p>
<p>• These are discrete representations- i.e: they treat language units (words, n-grams) as atomic units. This discreteness hampers the ability to capture relationship between words.
• The feature vectors are sparse and high-dimensional representations. The dimen‐
sionality increases with the size of the vocabulary, with most values being zero
for any vector. This hampers learning capability. Further, high-dimensionality
representation makes them computationally inefficient.
• They cannot handle OOV words.</p>
<p>Distributed Respresentations</p>
<p>Terminologies:</p>
<p>Distributional similarity: Meaning of the word can be understood by the context in which the word appears.</p>
<p>Connotation: meaning is defined by context
Denotation: Literal meaning of any word</p>
<p>Distributional hypothesis:
Words that occur similar context have similar meanings. For example: Guitar and Drum occur in similar context. Thus, they must have siimilar meaning. Now, following from
VSM, the meaning of a word is represented by the vector. Thus, if two words
often occur in similar context, then their corresponding representation vectors
must also be close to each other.</p>
<p>Distributional representation:</p>
<p>This refers to representation schemes that are obtained based on distribution of
words from the context in which the words appear. These schemes are based on
distributional hypotheses. Mathematically, distributional representation schemes use
high-dimensional vectors to represent words. These vectors are obtained from a
co-occurrence matrix that captures co-occurrence of word and context. The
dimension of this matrix is equal to the size of the vocabulary of the corpus. The
four schemes that we’ve seen so far—one-hot, bag of words, bag of n-grams, and
TF-IDF—all fall under the umbrella of distributional representation.</p>
<p>Distributed representation:
The vectors in distributional representation are
very high dimensional and sparse. This makes them computationally inefficient
and hampers learning. To alleviate this, distributed representation schemes sig‐
nificantly compress the dimensionality. This results in vectors that are compact
(i.e., low dimensional) and dense (i.e., hardly any zeros). The resulting vector
space is known as distributed representation.</p>
<p>Embedding: For the set of words in a corpus, embedding is a mapping between vector space
coming from distributional representation to vector space coming from dis‐
tributed representation.</p>
<p>Word Embeddings:</p>
<p>Word2vec ensures that the
learned word representations are low dimensional (vectors of dimensions 50–500,
instead of several thousands, as with previously studied representations in this chap‐
ter) and dense (that is, most values in these vectors are non-zero). Such representa‐
tions make ML tasks more tractable and efficient. Word2vec led to a lot of work (both
pure and applied) in the direction of learning text representations using neural
networks. These representations are also called “embeddings.” Let’s build an intuition
of how they work and how to use them to represent text.</p>
<p>Given a text corpus, the aim is to learn embeddings for every word in the corpus such
that the word vector in the embedding space best captures the meaning of the word.
To “derive” the meaning of the word, Word2vec uses distributional similarity and dis‐
tributional hypothesis. That is, it derives the meaning of a word from its context:
words that appear in its neighborhood in the text. So, if two different words (often)
occur in similar context, then it’s highly likely that their meanings are also similar.
Word2vec operationalizes this by projecting the meaning of the words in a vector
space where words with similar meanings will tend to cluster together, and words
with very different meanings are far from one another.</p>
<p>Architectural variants proposed in Word2Vec approach:</p>
<ol>
<li>
<p>Continuous bag of words (CBOW): In CBOW, the primary task is to build a language model that correctly pre‐
dicts the center word given the context words in which the center word appears.
What is a language model? It is a (statistical) model that tries to give a probability dis‐
tribution over sequences of words. Given a sentence of, say, m words, it assigns a
probability Pr(w 1 , w 2 , ….., w n ) to the whole sentence. The objective of a language
model is to assign probabilities in such a way that it gives high probability to “good”
sentences and low probabilities to “bad” sentences. By good, we mean sentences that
are semantically and syntactically correct. By bad, we mean sentences that are incor‐
rect—semantically or syntactically or both. So, for a sentence like “The cat jumped
over the dog,” it will try to assign a probability close to 1.0, whereas for a sentence like
“jumped over the the cat dog,” it tries to assign a probability close to 0.0.</p>
</li>
<li>
<p>SkipGram: SkipGram is very similar to CBOW, with some minor changes. In Skip‐
Gram, the task is to predict the context words from the center word. For our toy cor‐
pus with context size 2, using the center word “jumps,” we try to predict every word
in context—“brown,” “fox,” “over,” “the”. This constitutes
one step. SkipGram repeats this one step for every word in the corpus as the center
word.</p>
</li>
</ol>
<ul class="pa0">
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" >
  </a>
    <div>
<div class="ananke-socials">
  
    <a href="https://github.com/shikshya1" target="_blank" class="github ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="GitHub link" rel="noopener" aria-label="follow on GitHub——Opens in a new window">
      
        <span class="icon"><svg style="enable-background:new 0 0 512 512;" version="1.1" viewBox="0 0 512 512"  xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
  <path d="M256,32C132.3,32,32,134.8,32,261.7c0,101.5,64.2,187.5,153.2,217.9c11.2,2.1,15.3-5,15.3-11.1   c0-5.5-0.2-19.9-0.3-39.1c-62.3,13.9-75.5-30.8-75.5-30.8c-10.2-26.5-24.9-33.6-24.9-33.6c-20.3-14.3,1.5-14,1.5-14   c22.5,1.6,34.3,23.7,34.3,23.7c20,35.1,52.4,25,65.2,19.1c2-14.8,7.8-25,14.2-30.7c-49.7-5.8-102-25.5-102-113.5   c0-25.1,8.7-45.6,23-61.6c-2.3-5.8-10-29.2,2.2-60.8c0,0,18.8-6.2,61.6,23.5c17.9-5.1,37-7.6,56.1-7.7c19,0.1,38.2,2.6,56.1,7.7   c42.8-29.7,61.5-23.5,61.5-23.5c12.2,31.6,4.5,55,2.2,60.8c14.3,16.1,23,36.6,23,61.6c0,88.2-52.4,107.6-102.3,113.3   c8,7.1,15.2,21.1,15.2,42.5c0,30.7-0.3,55.5-0.3,63c0,6.1,4,13.3,15.4,11C415.9,449.1,480,363.1,480,261.7   C480,134.8,379.7,32,256,32z"/>
</svg>
</span>
      
<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000"  xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;"/>
</svg>
</span></a>
  
</div></div>
  </div>
</footer>

  </body>
</html>
