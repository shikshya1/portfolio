<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>scene on Shikshya Dahal</title><link>https://shikshya1.github.io/portfolio/tags/scene/</link><description>Recent content in scene on Shikshya Dahal</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 07 Apr 2020 10:58:08 -0400</lastBuildDate><atom:link href="https://shikshya1.github.io/portfolio/tags/scene/index.xml" rel="self" type="application/rss+xml"/><item><title>Day 1- Feature Scaling</title><link>https://shikshya1.github.io/portfolio/30-days-of-ml/project-1/</link><pubDate>Tue, 07 Apr 2020 10:58:08 -0400</pubDate><guid>https://shikshya1.github.io/portfolio/30-days-of-ml/project-1/</guid><description>Feature Scaling Feature scaling is one of the most important transformation in most of the ML projects. When one feature is on small range; say 0 to 10 while the other one is on a large range (suppose 0 to 10000); ML algorithms donot perform well. We have to scale the features so that both of them takes a comparable ranges of values to each other. In simpler terms, it means transforming data into a common range of values.</description></item><item><title>NLP 1: NLP pipelines</title><link>https://shikshya1.github.io/portfolio/nlp/project-1/</link><pubDate>Tue, 07 Apr 2020 10:58:08 -0400</pubDate><guid>https://shikshya1.github.io/portfolio/nlp/project-1/</guid><description>NLP :
&amp;ldquo;As we all know, dealing with natural language is hard. It is hard from the standpoint of the child, who must spend many years acquiring a language (compare this time span to that required for the acquisition of motor skills such as eating solids, walking, or swimming), it is hard for the adult language learner, it is hard for the scientist who attempts to model the relevant phenomena, and it is hard for the engineer who attempts to build systems that deal with natural language input or output.</description></item></channel></rss>