<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>Shikshya Dahal</title><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="The last theme you'll ever need. Maybe."><meta name=generator content="Hugo 0.104.3"><meta name=robots content="noindex, nofollow"><link rel=stylesheet href=/portfolio/ananke/css/main.min.css><link href=/portfolio/tags/index.xml rel=alternate type=application/rss+xml title="Shikshya Dahal"><link href=/portfolio/tags/index.xml rel=feed type=application/rss+xml title="Shikshya Dahal"><meta property="og:title" content="Tags"><meta property="og:description" content="The last theme you'll ever need. Maybe."><meta property="og:type" content="website"><meta property="og:url" content="https://shikshya1.github.io/portfolio/tags/"><meta property="og:site_name" content="Shikshya Dahal"><meta itemprop=name content="Tags"><meta itemprop=description content="The last theme you'll ever need. Maybe."><meta name=twitter:card content="summary"><meta name=twitter:title content="Tags"><meta name=twitter:description content="The last theme you'll ever need. Maybe."></head><body class="ma0 avenir bg-near-white"><header class="cover bg-top" style=background-image:url(https://shikshya1.github.io/portfolio/images/cpv.jpg)><div class=bg-black-60><nav class="pv3 ph3 ph4-ns" role=navigation><div class="flex-l justify-between items-center center"><a href=/portfolio/ class="f3 fw2 hover-white no-underline white-90 dib">Shikshya Dahal</a><div class="flex-l items-center"><ul class="pl0 mr3"><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/portfolio/about/ title="About page">About</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/portfolio/30-days-of-ml/ title="30 Days of ML page">30 Days of ML</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/portfolio/contact/ title="Contact page">Contact</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/portfolio/nlp/ title="NLP page">NLP</a></li></ul><div class=ananke-socials><a href=https://github.com/shikshya1 target=_blank class="github ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="GitHub link" rel=noopener aria-label="follow on GitHub——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 512 512" viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M256 32C132.3 32 32 134.8 32 261.7c0 101.5 64.2 187.5 153.2 217.9 11.2 2.1 15.3-5 15.3-11.1.0-5.5-.2-19.9-.3-39.1-62.3 13.9-75.5-30.8-75.5-30.8-10.2-26.5-24.9-33.6-24.9-33.6-20.3-14.3 1.5-14 1.5-14 22.5 1.6 34.3 23.7 34.3 23.7 20 35.1 52.4 25 65.2 19.1 2-14.8 7.8-25 14.2-30.7-49.7-5.8-102-25.5-102-113.5.0-25.1 8.7-45.6 23-61.6-2.3-5.8-10-29.2 2.2-60.8.0.0 18.8-6.2 61.6 23.5 17.9-5.1 37-7.6 56.1-7.7 19 .1 38.2 2.6 56.1 7.7 42.8-29.7 61.5-23.5 61.5-23.5 12.2 31.6 4.5 55 2.2 60.8 14.3 16.1 23 36.6 23 61.6.0 88.2-52.4 107.6-102.3 113.3 8 7.1 15.2 21.1 15.2 42.5.0 30.7-.3 55.5-.3 63 0 6.1 4 13.3 15.4 11C415.9 449.1 480 363.1 480 261.7 480 134.8 379.7 32 256 32z"/></svg></span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></span></a></div></div></div></nav><div class="tc-l pv4 pv6-l ph3 ph4-ns"><h1 class="f2 f-subheadline-l fw2 white-90 mb0 lh-title">Tags</h1></div></div></header><main class=pb7 role=main><article class="cf pa3 pa4-m pa4-l"><div class="measure-wide-l center f4 lh-copy nested-copy-line-height nested-links mid-gray"></div></article><div class="mw8 center"><section class=ph4><h2 class=f1><a href=/tags/bert class="link blue hover-black">Tag: bert</a></h2><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">30 Days of ML</span><h1 class="f3 near-black"><a href=/portfolio/30-days-of-ml/project-4/ class="link black dim">Day 4- Text Classification</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">Text Classification Classification categorizes data into one or more known classes. The data can be text, speech, image or numeric. Classification is used in wide range of applications across multiple domains such as: social media, healthcare, e-commerce etc. In today&rsquo;s post, I will be exploring text classification.
This notebook linked with this post demostrates how to use several ML algorigthms for the disaster classification task. The models built for this purpose in this notebook are:</div></div></div><h2 class=f1><a href=/tags/classification class="link blue hover-black">Tag: classification</a></h2><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">30 Days of ML</span><h1 class="f3 near-black"><a href=/portfolio/30-days-of-ml/project-4/ class="link black dim">Day 4- Text Classification</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">Text Classification Classification categorizes data into one or more known classes. The data can be text, speech, image or numeric. Classification is used in wide range of applications across multiple domains such as: social media, healthcare, e-commerce etc. In today&rsquo;s post, I will be exploring text classification.
This notebook linked with this post demostrates how to use several ML algorigthms for the disaster classification task. The models built for this purpose in this notebook are:</div></div></div><h2 class=f1><a href=/tags/lime class="link blue hover-black">Tag: lime</a></h2><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">30 Days of ML</span><h1 class="f3 near-black"><a href=/portfolio/30-days-of-ml/project-2/ class="link black dim">Day 2- Interpreting Classification Models</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">Interpreting Classification Models When we are building classification models; we are most of the time concerned about the performance criteria (how accurate the model is to classify the output labels). We donot look for explanations as to why it decided to classify the input to a certain class. But there are situations where questioning the decision of classification model becomes necessary.
Suppose we are building classifier that identifies spam emails. It identifies certain email as &lsquo;spam&rsquo; and moves it to spam folder rather than placing it in inbox folder without the need of human intervention.</div></div></div><h2 class=f1><a href=/tags/ml class="link blue hover-black">Tag: ml</a></h2><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">30 Days of ML</span><h1 class="f3 near-black"><a href=/portfolio/30-days-of-ml/project-1/ class="link black dim">Day 1- Feature Scaling</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">Feature Scaling Feature scaling is one of the most important transformation in most of the ML projects. When one feature is on small range; say 0 to 10 while the other one is on a large range (suppose 0 to 10000); ML algorithms donot perform well. We have to scale the features so that both of them takes a comparable ranges of values to each other. In simpler terms, it means transforming data into a common range of values.</div></div></div><h2 class=f1><a href=/tags/ner class="link blue hover-black">Tag: ner</a></h2><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">30 Days of ML</span><h1 class="f3 near-black"><a href=/portfolio/30-days-of-ml/project-5/ class="link black dim">Day 5- NER</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">Named Entity Recognition NER refers to the IE task of identifying the entities in a document. Entities are typically names of persons, locations, and organizations, and other specialized strings, such as money expressions, dates, products, names/numbers of laws or articles, and so on. NER is an important step in the pipeline of several NLP applications involving information extraction.
A simple approach to building an NER system is to maintain a large collection of person/organization/location names that are the most relevant to our company (e.</div></div></div><h2 class=f1><a href=/tags/normalization class="link blue hover-black">Tag: normalization</a></h2><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">30 Days of ML</span><h1 class="f3 near-black"><a href=/portfolio/30-days-of-ml/project-1/ class="link black dim">Day 1- Feature Scaling</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">Feature Scaling Feature scaling is one of the most important transformation in most of the ML projects. When one feature is on small range; say 0 to 10 while the other one is on a large range (suppose 0 to 10000); ML algorithms donot perform well. We have to scale the features so that both of them takes a comparable ranges of values to each other. In simpler terms, it means transforming data into a common range of values.</div></div></div><h2 class=f1><a href=/tags/pca class="link blue hover-black">Tag: pca</a></h2><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">30 Days of ML</span><h1 class="f3 near-black"><a href=/portfolio/30-days-of-ml/project-3/ class="link black dim">Day 3- t-SNE</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">t-Distributed Stochastic Neighbor Embedding (t-SNE) t-SNE is a unsupervised, non-parametric (non-linear) dimensionality reduction method used for visualization and exploration of high-dimensional datasets. It gives an idea of how data is arranged in higher dimension. It&rsquo;s hard for us to visualize data beyond 3 dimension. Standard visualization methods can usually capture one or two variable at a time. In such cases, dimension reduction algorithm can help to analyze the pattern in the data.</div></div></div><h2 class=f1><a href=/tags/scene class="link blue hover-black">Tag: scene</a></h2><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">NLP</span><h1 class="f3 near-black"><a href=/portfolio/nlp/project-1/ class="link black dim">NLP 1: NLP pipelines</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">NLP :
&ldquo;As we all know, dealing with natural language is hard. It is hard from the standpoint of the child, who must spend many years acquiring a language (compare this time span to that required for the acquisition of motor skills such as eating solids, walking, or swimming), it is hard for the adult language learner, it is hard for the scientist who attempts to model the relevant phenomena, and it is hard for the engineer who attempts to build systems that deal with natural language input or output.</div></div></div><h2 class=f1><a href=/tags/shap class="link blue hover-black">Tag: shap</a></h2><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">30 Days of ML</span><h1 class="f3 near-black"><a href=/portfolio/30-days-of-ml/project-2/ class="link black dim">Day 2- Interpreting Classification Models</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">Interpreting Classification Models When we are building classification models; we are most of the time concerned about the performance criteria (how accurate the model is to classify the output labels). We donot look for explanations as to why it decided to classify the input to a certain class. But there are situations where questioning the decision of classification model becomes necessary.
Suppose we are building classifier that identifies spam emails. It identifies certain email as &lsquo;spam&rsquo; and moves it to spam folder rather than placing it in inbox folder without the need of human intervention.</div></div></div><h2 class=f1><a href=/tags/svm class="link blue hover-black">Tag: svm</a></h2><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">30 Days of ML</span><h1 class="f3 near-black"><a href=/portfolio/30-days-of-ml/project-4/ class="link black dim">Day 4- Text Classification</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">Text Classification Classification categorizes data into one or more known classes. The data can be text, speech, image or numeric. Classification is used in wide range of applications across multiple domains such as: social media, healthcare, e-commerce etc. In today&rsquo;s post, I will be exploring text classification.
This notebook linked with this post demostrates how to use several ML algorigthms for the disaster classification task. The models built for this purpose in this notebook are:</div></div></div><h2 class=f1><a href=/tags/t-sne class="link blue hover-black">Tag: t-sne</a></h2><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">30 Days of ML</span><h1 class="f3 near-black"><a href=/portfolio/30-days-of-ml/project-3/ class="link black dim">Day 3- t-SNE</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">t-Distributed Stochastic Neighbor Embedding (t-SNE) t-SNE is a unsupervised, non-parametric (non-linear) dimensionality reduction method used for visualization and exploration of high-dimensional datasets. It gives an idea of how data is arranged in higher dimension. It&rsquo;s hard for us to visualize data beyond 3 dimension. Standard visualization methods can usually capture one or two variable at a time. In such cases, dimension reduction algorithm can help to analyze the pattern in the data.</div></div></div></section></div></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3"></a><div><div class=ananke-socials><a href=https://github.com/shikshya1 target=_blank class="github ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="GitHub link" rel=noopener aria-label="follow on GitHub——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 512 512" viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M256 32C132.3 32 32 134.8 32 261.7c0 101.5 64.2 187.5 153.2 217.9 11.2 2.1 15.3-5 15.3-11.1.0-5.5-.2-19.9-.3-39.1-62.3 13.9-75.5-30.8-75.5-30.8-10.2-26.5-24.9-33.6-24.9-33.6-20.3-14.3 1.5-14 1.5-14 22.5 1.6 34.3 23.7 34.3 23.7 20 35.1 52.4 25 65.2 19.1 2-14.8 7.8-25 14.2-30.7-49.7-5.8-102-25.5-102-113.5.0-25.1 8.7-45.6 23-61.6-2.3-5.8-10-29.2 2.2-60.8.0.0 18.8-6.2 61.6 23.5 17.9-5.1 37-7.6 56.1-7.7 19 .1 38.2 2.6 56.1 7.7 42.8-29.7 61.5-23.5 61.5-23.5 12.2 31.6 4.5 55 2.2 60.8 14.3 16.1 23 36.6 23 61.6.0 88.2-52.4 107.6-102.3 113.3 8 7.1 15.2 21.1 15.2 42.5.0 30.7-.3 55.5-.3 63 0 6.1 4 13.3 15.4 11C415.9 449.1 480 363.1 480 261.7 480 134.8 379.7 32 256 32z"/></svg></span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></span></a></div></div></div></footer></body></html>