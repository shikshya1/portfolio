<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>Shikshya Dahal</title><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="The last theme you'll ever need. Maybe."><meta name=generator content="Hugo 0.108.0"><meta name=robots content="noindex, nofollow"><link rel=stylesheet href=/portfolio/ananke/css/main.min.css><link href=/portfolio/tags/index.xml rel=alternate type=application/rss+xml title="Shikshya Dahal"><link href=/portfolio/tags/index.xml rel=feed type=application/rss+xml title="Shikshya Dahal"><meta property="og:title" content="Tags"><meta property="og:description" content="The last theme you'll ever need. Maybe."><meta property="og:type" content="website"><meta property="og:url" content="https://shikshya1.github.io/portfolio/tags/"><meta property="og:site_name" content="Shikshya Dahal"><meta itemprop=name content="Tags"><meta itemprop=description content="The last theme you'll ever need. Maybe."><meta name=twitter:card content="summary"><meta name=twitter:title content="Tags"><meta name=twitter:description content="The last theme you'll ever need. Maybe."></head><body class="ma0 avenir bg-near-white"><header class="cover bg-top" style=background-image:url(https://shikshya1.github.io/portfolio/images/cpv.jpg)><div class=bg-black-60><nav class="pv3 ph3 ph4-ns" role=navigation><div class="flex-l justify-between items-center center"><a href=/portfolio/ class="f3 fw2 hover-white no-underline white-90 dib">Shikshya Dahal</a><div class="flex-l items-center"><ul class="pl0 mr3"><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/portfolio/about/ title="About page">About</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/portfolio/aws/ title="AWS page">AWS</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/portfolio/ml/ title="ML Basics page">ML Basics</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/portfolio/nlp/ title="NLP page">NLP</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/portfolio/projects/ title="Projects page">Projects</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/portfolio/statistics/ title="Statistics page">Statistics</a></li></ul><div class=ananke-socials><a href=https://github.com/shikshya1 target=_blank class="github ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="GitHub link" rel=noopener aria-label="follow on GitHub——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 512 512" viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M256 32C132.3 32 32 134.8 32 261.7c0 101.5 64.2 187.5 153.2 217.9 11.2 2.1 15.3-5 15.3-11.1.0-5.5-.2-19.9-.3-39.1-62.3 13.9-75.5-30.8-75.5-30.8-10.2-26.5-24.9-33.6-24.9-33.6-20.3-14.3 1.5-14 1.5-14 22.5 1.6 34.3 23.7 34.3 23.7 20 35.1 52.4 25 65.2 19.1 2-14.8 7.8-25 14.2-30.7-49.7-5.8-102-25.5-102-113.5.0-25.1 8.7-45.6 23-61.6-2.3-5.8-10-29.2 2.2-60.8.0.0 18.8-6.2 61.6 23.5 17.9-5.1 37-7.6 56.1-7.7 19 .1 38.2 2.6 56.1 7.7 42.8-29.7 61.5-23.5 61.5-23.5 12.2 31.6 4.5 55 2.2 60.8 14.3 16.1 23 36.6 23 61.6.0 88.2-52.4 107.6-102.3 113.3 8 7.1 15.2 21.1 15.2 42.5.0 30.7-.3 55.5-.3 63 0 6.1 4 13.3 15.4 11C415.9 449.1 480 363.1 480 261.7 480 134.8 379.7 32 256 32z"/></svg></span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></span></a></div></div></div></nav><div class="tc-l pv4 pv6-l ph3 ph4-ns"><h1 class="f2 f-subheadline-l fw2 white-90 mb0 lh-title">Tags</h1></div></div></header><main class=pb7 role=main><article class="cf pa3 pa4-m pa4-l"><div class="measure-wide-l center f4 lh-copy nested-copy-line-height nested-links mid-gray"></div></article><div class="mw8 center"><section class=ph4><h2 class=f1><a href=/tags/aws-lambda class="link blue hover-black">Tag: aws-lambda</a></h2><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">AWS</span><h1 class="f3 near-black"><a href=/portfolio/aws/project-2/ class="link black dim">Create a trigger to upload records from csv to DynamoDB when csv file is inserted on S3 via Lambda function using Serverless Framework</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">This project demonstrates how to process a csv file when inserted on s3 bucket and use the data to populate DynamoDB table. DynamoDB is a fully managed, serverless, key-value NoSQL database designed to run high-performance applications at any scale.I am only using DynamoDB in this project for test purpose as it is a bad choice when we are dealing with small size of data.
Steps Initialize a aws-python template from serverless framework We then need to import boto3 (Python SDK that allows us to interact with DynamoDB and S3), pandas and s3fs (to read the csv file) library.</div></div></div><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">AWS</span><h1 class="f3 near-black"><a href=/portfolio/aws/project-1/ class="link black dim">Deploy lambda function with external python packages using serverless framework.</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">Lambda Function & Layers: AWS Lambda is a computing service that allows us to run our code without the overhead of configuring and managing a server. It can be scaled automatically based on the request and the charge is calculated only for the compute time. Lambda function can be deployed using .zip file archive that contains function and the dependency or using container image. Lambda layers can be used to package external dependencies that is required to run lambda function.</div></div></div><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">AWS</span><h1 class="f3 near-black"><a href=/portfolio/aws/project-2-copy/ class="link black dim">Mount AWS EFS volume into AWS Lambda with the Serverless Framework</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">One of the biggest challenge while deploying machine learning applications in AWS Lambda is the limited size of deployment package. It becomes more challenging when you are dealing with &lsquo;state of art&rsquo; models like BERT. There are several ways to compress the models and use it in production. One of the technique is to mount EFS to the serverless function.
EFS is built to scale on demand to petabytes of data, growing and shrinking automatically as files are written and deleted.</div></div></div><h2 class=f1><a href=/tags/bert class="link blue hover-black">Tag: bert</a></h2><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">NLP</span><h1 class="f3 near-black"><a href=/portfolio/nlp/project-6/ class="link black dim">Measuring Distance</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">Eucledian Distance Eucledian distance measures the distance between two points.
def eucledian_dist(a,b): return np.sqrt(np.sum(np.square(a - b))) Cosine similarity Cosine similarity measures how similar the documents are irrespective of their size. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. It is a judgment of orientation and not magnitude. The cosine similarity is advantageous because even if the two similar documents are far apart by the Euclidean distance (due to the size of the document), chances are they may still be oriented closer together.</div></div></div><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">NLP</span><h1 class="f3 near-black"><a href=/portfolio/nlp/project-3/ class="link black dim">Text Classification</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">Classification categorizes data into one or more known classes. The data can be text, speech, image or numeric. Classification is used in wide range of applications across multiple domains such as: social media, healthcare, e-commerce etc. In today&rsquo;s post, I will be exploring text classification.
This notebook linked with this post demostrates how to use several ML algorigthms for the disaster classification task. The models built for this purpose in this notebook are:</div></div></div><h2 class=f1><a href=/tags/bertopic class="link blue hover-black">Tag: bertopic</a></h2><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">Projects</span><h1 class="f3 near-black"><a href=/portfolio/projects/topic_modeling/ class="link black dim">Topic modeling of Depression posts scraped from reddit</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">Topic modeling is a statistical modeling that can be used to discover hidden (latent) themes from the collection of documents. Having information about the problems and opinions about a certain product or services can be very important for businesses. But discovering themes in a large mine of online reviews or comments in the internet can be a expensive process. Thus, several algorithms like NMF, LDA, BERT can be employed for this purpose.</div></div></div><h2 class=f1><a href=/tags/classification class="link blue hover-black">Tag: classification</a></h2><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">NLP</span><h1 class="f3 near-black"><a href=/portfolio/nlp/project-3/ class="link black dim">Text Classification</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">Classification categorizes data into one or more known classes. The data can be text, speech, image or numeric. Classification is used in wide range of applications across multiple domains such as: social media, healthcare, e-commerce etc. In today&rsquo;s post, I will be exploring text classification.
This notebook linked with this post demostrates how to use several ML algorigthms for the disaster classification task. The models built for this purpose in this notebook are:</div></div></div><h2 class=f1><a href=/tags/clustering class="link blue hover-black">Tag: clustering</a></h2><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">ML Basics</span><h1 class="f3 near-black"><a href=/portfolio/ml/project-3/ class="link black dim">Clustering Amazon books based on book title</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">Dataset: The dataset contains 946 books obtained from scraping Amazon books.
Reference: https://www.kaggle.com/datasets/die9origephit/amazon-data-science-books
Read DataFrame
df= pd.read_csv('CSVPATH') Representing text using TF-IDF
from sklearn.feature_extraction.text import TfidfVectorizer vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,2)) X = vectorizer.fit_transform(df["title"]) Clustering: For the clustering, KMeans is used. KMeans is an unsupervised learning method that clusters dataset into &lsquo;k&rsquo; different clusters. Each sample is assigned to the cluster with the nearest mean and then the means are updated during iterative optimization process.</div></div></div><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">Projects</span><h1 class="f3 near-black"><a href=/portfolio/projects/customer_segmentation/ class="link black dim">Customer segmentation of retail dataset using kmeans clustering algorithm</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">Customer Segmentation
customer segmentation is a process of dividing customers into different groups based on their characteristics. This can be useful for mall management in order to better understand their customers and tailor marketing strategies to different groups. One popular method for customer segmentation is k-means clustering, which is a machine learning technique that groups similar data points together. In this article, we will be discussing how to use the k-means algorithm in Python to perform customer segmentation for a mall.</div></div></div><h2 class=f1><a href=/tags/cosine-similarity class="link blue hover-black">Tag: cosine-similarity</a></h2><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">NLP</span><h1 class="f3 near-black"><a href=/portfolio/nlp/project-6/ class="link black dim">Measuring Distance</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">Eucledian Distance Eucledian distance measures the distance between two points.
def eucledian_dist(a,b): return np.sqrt(np.sum(np.square(a - b))) Cosine similarity Cosine similarity measures how similar the documents are irrespective of their size. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. It is a judgment of orientation and not magnitude. The cosine similarity is advantageous because even if the two similar documents are far apart by the Euclidean distance (due to the size of the document), chances are they may still be oriented closer together.</div></div></div><h2 class=f1><a href=/tags/eucledian-distance class="link blue hover-black">Tag: eucledian-distance</a></h2><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">NLP</span><h1 class="f3 near-black"><a href=/portfolio/nlp/project-6/ class="link black dim">Measuring Distance</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">Eucledian Distance Eucledian distance measures the distance between two points.
def eucledian_dist(a,b): return np.sqrt(np.sum(np.square(a - b))) Cosine similarity Cosine similarity measures how similar the documents are irrespective of their size. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. It is a judgment of orientation and not magnitude. The cosine similarity is advantageous because even if the two similar documents are far apart by the Euclidean distance (due to the size of the document), chances are they may still be oriented closer together.</div></div></div><h2 class=f1><a href=/tags/k-means class="link blue hover-black">Tag: k-means</a></h2><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">Projects</span><h1 class="f3 near-black"><a href=/portfolio/projects/customer_segmentation/ class="link black dim">Customer segmentation of retail dataset using kmeans clustering algorithm</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">Customer Segmentation
customer segmentation is a process of dividing customers into different groups based on their characteristics. This can be useful for mall management in order to better understand their customers and tailor marketing strategies to different groups. One popular method for customer segmentation is k-means clustering, which is a machine learning technique that groups similar data points together. In this article, we will be discussing how to use the k-means algorithm in Python to perform customer segmentation for a mall.</div></div></div><h2 class=f1><a href=/tags/kmeans class="link blue hover-black">Tag: kmeans</a></h2><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">ML Basics</span><h1 class="f3 near-black"><a href=/portfolio/ml/project-3/ class="link black dim">Clustering Amazon books based on book title</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">Dataset: The dataset contains 946 books obtained from scraping Amazon books.
Reference: https://www.kaggle.com/datasets/die9origephit/amazon-data-science-books
Read DataFrame
df= pd.read_csv('CSVPATH') Representing text using TF-IDF
from sklearn.feature_extraction.text import TfidfVectorizer vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,2)) X = vectorizer.fit_transform(df["title"]) Clustering: For the clustering, KMeans is used. KMeans is an unsupervised learning method that clusters dataset into &lsquo;k&rsquo; different clusters. Each sample is assigned to the cluster with the nearest mean and then the means are updated during iterative optimization process.</div></div></div><h2 class=f1><a href=/tags/lda class="link blue hover-black">Tag: lda</a></h2><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">Projects</span><h1 class="f3 near-black"><a href=/portfolio/projects/topic_modeling/ class="link black dim">Topic modeling of Depression posts scraped from reddit</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">Topic modeling is a statistical modeling that can be used to discover hidden (latent) themes from the collection of documents. Having information about the problems and opinions about a certain product or services can be very important for businesses. But discovering themes in a large mine of online reviews or comments in the internet can be a expensive process. Thus, several algorithms like NMF, LDA, BERT can be employed for this purpose.</div></div></div><h2 class=f1><a href=/tags/lime class="link blue hover-black">Tag: lime</a></h2><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">NLP</span><h1 class="f3 near-black"><a href=/portfolio/nlp/project-2/ class="link black dim">Interpreting Classification Models</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">When we are building classification models; we are most of the time concerned about the performance criteria (how accurate the model is to classify the output labels). We donot look for explanations as to why it decided to classify the input to a certain class. But there are situations where questioning the decision of classification model becomes necessary.
Suppose we are building classifier that identifies spam emails. It identifies certain email as &lsquo;spam&rsquo; and moves it to spam folder rather than placing it in inbox folder without the need of human intervention.</div></div></div><h2 class=f1><a href=/tags/ml class="link blue hover-black">Tag: ml</a></h2><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">ML Basics</span><h1 class="f3 near-black"><a href=/portfolio/ml/project-1/ class="link black dim">Feature Scaling</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">Feature scaling is one of the most important transformation in most of the ML projects. When one feature is on small range; say 0 to 10 while the other one is on a large range (suppose 0 to 10000); ML algorithms donot perform well. We have to scale the features so that both of them takes a comparable ranges of values to each other. In simpler terms, it means transforming data into a common range of values.</div></div></div><h2 class=f1><a href=/tags/ner class="link blue hover-black">Tag: ner</a></h2><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">NLP</span><h1 class="f3 near-black"><a href=/portfolio/nlp/project-4/ class="link black dim">NER</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">NER refers to the IE task of identifying the entities in a document. Entities are typically names of persons, locations, and organizations, and other specialized strings, such as money expressions, dates, products, names/numbers of laws or articles, and so on. NER is an important step in the pipeline of several NLP applications involving information extraction.
A simple approach to building an NER system is to maintain a large collection of person/organization/location names that are the most relevant to our company (e.</div></div></div><h2 class=f1><a href=/tags/normalization class="link blue hover-black">Tag: normalization</a></h2><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">ML Basics</span><h1 class="f3 near-black"><a href=/portfolio/ml/project-1/ class="link black dim">Feature Scaling</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">Feature scaling is one of the most important transformation in most of the ML projects. When one feature is on small range; say 0 to 10 while the other one is on a large range (suppose 0 to 10000); ML algorithms donot perform well. We have to scale the features so that both of them takes a comparable ranges of values to each other. In simpler terms, it means transforming data into a common range of values.</div></div></div><h2 class=f1><a href=/tags/pca class="link blue hover-black">Tag: pca</a></h2><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">ML Basics</span><h1 class="f3 near-black"><a href=/portfolio/ml/project-2/ class="link black dim">t-SNE</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">t-Distributed Stochastic Neighbor Embedding (t-SNE) is a unsupervised, non-parametric (non-linear) dimensionality reduction method used for visualization and exploration of high-dimensional datasets. It gives an idea of how data is arranged in higher dimension. It&rsquo;s hard for us to visualize data beyond 3 dimension. Standard visualization methods can usually capture one or two variable at a time. In such cases, dimension reduction algorithm can help to analyze the pattern in the data.</div></div></div><h2 class=f1><a href=/tags/scene class="link blue hover-black">Tag: scene</a></h2><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">NLP</span><h1 class="f3 near-black"><a href=/portfolio/nlp/project-1/ class="link black dim">NLP 1: NLP pipelines</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">NLP :
&ldquo;As we all know, dealing with natural language is hard. It is hard from the standpoint of the child, who must spend many years acquiring a language (compare this time span to that required for the acquisition of motor skills such as eating solids, walking, or swimming), it is hard for the adult language learner, it is hard for the scientist who attempts to model the relevant phenomena, and it is hard for the engineer who attempts to build systems that deal with natural language input or output.</div></div></div><h2 class=f1><a href=/tags/serverless class="link blue hover-black">Tag: serverless</a></h2><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">AWS</span><h1 class="f3 near-black"><a href=/portfolio/aws/project-2/ class="link black dim">Create a trigger to upload records from csv to DynamoDB when csv file is inserted on S3 via Lambda function using Serverless Framework</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">This project demonstrates how to process a csv file when inserted on s3 bucket and use the data to populate DynamoDB table. DynamoDB is a fully managed, serverless, key-value NoSQL database designed to run high-performance applications at any scale.I am only using DynamoDB in this project for test purpose as it is a bad choice when we are dealing with small size of data.
Steps Initialize a aws-python template from serverless framework We then need to import boto3 (Python SDK that allows us to interact with DynamoDB and S3), pandas and s3fs (to read the csv file) library.</div></div></div><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">AWS</span><h1 class="f3 near-black"><a href=/portfolio/aws/project-1/ class="link black dim">Deploy lambda function with external python packages using serverless framework.</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">Lambda Function & Layers: AWS Lambda is a computing service that allows us to run our code without the overhead of configuring and managing a server. It can be scaled automatically based on the request and the charge is calculated only for the compute time. Lambda function can be deployed using .zip file archive that contains function and the dependency or using container image. Lambda layers can be used to package external dependencies that is required to run lambda function.</div></div></div><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">AWS</span><h1 class="f3 near-black"><a href=/portfolio/aws/project-2-copy/ class="link black dim">Mount AWS EFS volume into AWS Lambda with the Serverless Framework</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">One of the biggest challenge while deploying machine learning applications in AWS Lambda is the limited size of deployment package. It becomes more challenging when you are dealing with &lsquo;state of art&rsquo; models like BERT. There are several ways to compress the models and use it in production. One of the technique is to mount EFS to the serverless function.
EFS is built to scale on demand to petabytes of data, growing and shrinking automatically as files are written and deleted.</div></div></div><h2 class=f1><a href=/tags/shap class="link blue hover-black">Tag: shap</a></h2><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">NLP</span><h1 class="f3 near-black"><a href=/portfolio/nlp/project-2/ class="link black dim">Interpreting Classification Models</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">When we are building classification models; we are most of the time concerned about the performance criteria (how accurate the model is to classify the output labels). We donot look for explanations as to why it decided to classify the input to a certain class. But there are situations where questioning the decision of classification model becomes necessary.
Suppose we are building classifier that identifies spam emails. It identifies certain email as &lsquo;spam&rsquo; and moves it to spam folder rather than placing it in inbox folder without the need of human intervention.</div></div></div><h2 class=f1><a href=/tags/statistics class="link blue hover-black">Tag: statistics</a></h2><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">Statistics</span><h1 class="f3 near-black"><a href=/portfolio/statistics/project-1/ class="link black dim">Exploratory Data Analysis</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">Statistics is the study of how to collect, organize, analyze and interpret numerical information and data.
There are two major terms repeated frequently in the statistics domain: population and sample.
Population: It refers to the group of people or objects with common theme. Example: Technical team at Target. Sample: It refers to the small portion of population. Example: only UX designers at target. Exploratory data analysis (EDA): EDA is one of the most important step in any data science project.</div></div></div><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">Statistics</span><h1 class="f3 near-black"><a href=/portfolio/statistics/project-3/ class="link black dim">Exploratory Data Analysis</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height"></div></div></div><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">Statistics</span><h1 class="f3 near-black"><a href=/portfolio/statistics/project-2/ class="link black dim">Sampling and bias</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">The rise of big data in today&rsquo;s world might raise a misconception that we donot need sampling anymore. But it is even more relevant in today&rsquo;s world as we have to deal with data from variety of sources and to minimize bias.It is impractical and unnecessary to measure the whole population. It also helps to save resources.
Sampling frame: List of individuals from which the sample is selected.
Types:
Simple random sampling (SRS): A simple random sample of n measurements from a population is a subset of the population is a subset of the population selected in such a way that sample of size n from the population has an equal chance of being selected.</div></div></div><h2 class=f1><a href=/tags/svm class="link blue hover-black">Tag: svm</a></h2><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">NLP</span><h1 class="f3 near-black"><a href=/portfolio/nlp/project-3/ class="link black dim">Text Classification</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">Classification categorizes data into one or more known classes. The data can be text, speech, image or numeric. Classification is used in wide range of applications across multiple domains such as: social media, healthcare, e-commerce etc. In today&rsquo;s post, I will be exploring text classification.
This notebook linked with this post demostrates how to use several ML algorigthms for the disaster classification task. The models built for this purpose in this notebook are:</div></div></div><h2 class=f1><a href=/tags/t-sne class="link blue hover-black">Tag: t-sne</a></h2><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">ML Basics</span><h1 class="f3 near-black"><a href=/portfolio/ml/project-2/ class="link black dim">t-SNE</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">t-Distributed Stochastic Neighbor Embedding (t-SNE) is a unsupervised, non-parametric (non-linear) dimensionality reduction method used for visualization and exploration of high-dimensional datasets. It gives an idea of how data is arranged in higher dimension. It&rsquo;s hard for us to visualize data beyond 3 dimension. Standard visualization methods can usually capture one or two variable at a time. In such cases, dimension reduction algorithm can help to analyze the pattern in the data.</div></div></div><h2 class=f1><a href=/tags/word2vec class="link blue hover-black">Tag: word2vec</a></h2><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">NLP</span><h1 class="f3 near-black"><a href=/portfolio/nlp/project-5/ class="link black dim">Text Representation</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">Maps each word in the vocabulary(V) of the text corpus to a unique ID (integer value), then represent each sentence or document in the corpus as a V-dimensional vector.
One-Hot encoding: In one-hot encoding, each word w in the corpus vocabulary is given a unique integer ID w id that is between 1 and |V|, where V is the set of the corpus vocabulary. Each word is then represented by a V-dimensional binary vector of 0s and 1s.</div></div></div></section></div></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3"></a><div><div class=ananke-socials><a href=https://github.com/shikshya1 target=_blank class="github ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="GitHub link" rel=noopener aria-label="follow on GitHub——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 512 512" viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M256 32C132.3 32 32 134.8 32 261.7c0 101.5 64.2 187.5 153.2 217.9 11.2 2.1 15.3-5 15.3-11.1.0-5.5-.2-19.9-.3-39.1-62.3 13.9-75.5-30.8-75.5-30.8-10.2-26.5-24.9-33.6-24.9-33.6-20.3-14.3 1.5-14 1.5-14 22.5 1.6 34.3 23.7 34.3 23.7 20 35.1 52.4 25 65.2 19.1 2-14.8 7.8-25 14.2-30.7-49.7-5.8-102-25.5-102-113.5.0-25.1 8.7-45.6 23-61.6-2.3-5.8-10-29.2 2.2-60.8.0.0 18.8-6.2 61.6 23.5 17.9-5.1 37-7.6 56.1-7.7 19 .1 38.2 2.6 56.1 7.7 42.8-29.7 61.5-23.5 61.5-23.5 12.2 31.6 4.5 55 2.2 60.8 14.3 16.1 23 36.6 23 61.6.0 88.2-52.4 107.6-102.3 113.3 8 7.1 15.2 21.1 15.2 42.5.0 30.7-.3 55.5-.3 63 0 6.1 4 13.3 15.4 11C415.9 449.1 480 363.1 480 261.7 480 134.8 379.7 32 256 32z"/></svg></span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></span></a></div></div></div></footer></body></html>