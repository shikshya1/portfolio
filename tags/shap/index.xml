<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>SHAP on Shikshya Dahal</title><link>https://shikshya1.github.io/portfolio/tags/shap/</link><description>Recent content in SHAP on Shikshya Dahal</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sun, 02 Oct 2022 10:58:08 -0400</lastBuildDate><atom:link href="https://shikshya1.github.io/portfolio/tags/shap/index.xml" rel="self" type="application/rss+xml"/><item><title>Day 2- Interpreting Classification Models</title><link>https://shikshya1.github.io/portfolio/ml/project-2/</link><pubDate>Sun, 02 Oct 2022 10:58:08 -0400</pubDate><guid>https://shikshya1.github.io/portfolio/ml/project-2/</guid><description>Interpreting Classification Models When we are building classification models; we are most of the time concerned about the performance criteria (how accurate the model is to classify the output labels). We donot look for explanations as to why it decided to classify the input to a certain class. But there are situations where questioning the decision of classification model becomes necessary.
Suppose we are building classifier that identifies spam emails. It identifies certain email as &amp;lsquo;spam&amp;rsquo; and moves it to spam folder rather than placing it in inbox folder without the need of human intervention.</description></item></channel></rss>