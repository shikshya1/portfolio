<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Word2Vec on Shikshya Dahal</title><link>https://shikshya1.github.io/portfolio/tags/word2vec/</link><description>Recent content in Word2Vec on Shikshya Dahal</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 06 Oct 2022 10:58:08 -0400</lastBuildDate><atom:link href="https://shikshya1.github.io/portfolio/tags/word2vec/index.xml" rel="self" type="application/rss+xml"/><item><title>Text Representation</title><link>https://shikshya1.github.io/portfolio/nlp/project-5/</link><pubDate>Thu, 06 Oct 2022 10:58:08 -0400</pubDate><guid>https://shikshya1.github.io/portfolio/nlp/project-5/</guid><description>Text Representation Maps each word in the vocabulary(V) of the text corpus to a unique ID (integer value), then represent each sentence or document in the corpus as a V-dimensional vector.
One-Hot encoding: In one-hot encoding, each word w in the corpus vocabulary is given a unique integer ID w id that is between 1 and |V|, where V is the set of the corpus vocabulary. Each word is then represented by a V-dimensional binary vector of 0s and 1s.</description></item></channel></rss>