<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>ML Basics on Shikshya Dahal</title><link>https://shikshya1.github.io/portfolio/ml/</link><description>Recent content in ML Basics on Shikshya Dahal</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 02 Mar 2017 12:00:00 -0500</lastBuildDate><atom:link href="https://shikshya1.github.io/portfolio/ml/index.xml" rel="self" type="application/rss+xml"/><item><title>Clustering Amazon books based on book title</title><link>https://shikshya1.github.io/portfolio/ml/project-3/</link><pubDate>Sun, 01 Jan 2023 10:58:08 -0400</pubDate><guid>https://shikshya1.github.io/portfolio/ml/project-3/</guid><description>Dataset: The dataset contains 946 books obtained from scraping Amazon books.
Reference: https://www.kaggle.com/datasets/die9origephit/amazon-data-science-books
Read DataFrame
df= pd.read_csv(&amp;#39;CSVPATH&amp;#39;) Representing text using TF-IDF
from sklearn.feature_extraction.text import TfidfVectorizer vectorizer = TfidfVectorizer(stop_words=&amp;#39;english&amp;#39;, ngram_range=(1,2)) X = vectorizer.fit_transform(df[&amp;#34;title&amp;#34;]) Clustering: For the clustering, KMeans is used. KMeans is an unsupervised learning method that clusters dataset into &amp;lsquo;k&amp;rsquo; different clusters. Each sample is assigned to the cluster with the nearest mean and then the means are updated during iterative optimization process.</description></item><item><title>t-SNE</title><link>https://shikshya1.github.io/portfolio/ml/project-2/</link><pubDate>Mon, 03 Oct 2022 10:58:08 -0400</pubDate><guid>https://shikshya1.github.io/portfolio/ml/project-2/</guid><description>t-Distributed Stochastic Neighbor Embedding (t-SNE) is a unsupervised, non-parametric (non-linear) dimensionality reduction method used for visualization and exploration of high-dimensional datasets. It gives an idea of how data is arranged in higher dimension. It&amp;rsquo;s hard for us to visualize data beyond 3 dimension. Standard visualization methods can usually capture one or two variable at a time. In such cases, dimension reduction algorithm can help to analyze the pattern in the data.</description></item><item><title>Feature Scaling</title><link>https://shikshya1.github.io/portfolio/ml/project-1/</link><pubDate>Sat, 01 Oct 2022 10:58:08 -0400</pubDate><guid>https://shikshya1.github.io/portfolio/ml/project-1/</guid><description>Feature scaling is one of the most important transformation in most of the ML projects. When one feature is on small range; say 0 to 10 while the other one is on a large range (suppose 0 to 10000); ML algorithms donot perform well. We have to scale the features so that both of them takes a comparable ranges of values to each other. In simpler terms, it means transforming data into a common range of values.</description></item></channel></rss>