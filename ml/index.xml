<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>ML Basics on Shikshya Dahal</title><link>https://shikshya1.github.io/portfolio/ml/</link><description>Recent content in ML Basics on Shikshya Dahal</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 02 Mar 2017 12:00:00 -0500</lastBuildDate><atom:link href="https://shikshya1.github.io/portfolio/ml/index.xml" rel="self" type="application/rss+xml"/><item><title>Day 7- Measuring Distance</title><link>https://shikshya1.github.io/portfolio/ml/project-7/</link><pubDate>Fri, 07 Oct 2022 10:58:08 -0400</pubDate><guid>https://shikshya1.github.io/portfolio/ml/project-7/</guid><description>Measuring Distance Eucledian Distance Eucledian distance measures the distance between two points.
Cosine similarity Cosine similarity measures how similar the documents are irrespective of their size. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. It is a judgment of orientation and not magnitude. The cosine similarity is advantageous because even if the two similar documents are far apart by the Euclidean distance (due to the size of the document), chances are they may still be oriented closer together.</description></item><item><title>Day 6- Text Representation</title><link>https://shikshya1.github.io/portfolio/ml/project-6/</link><pubDate>Thu, 06 Oct 2022 10:58:08 -0400</pubDate><guid>https://shikshya1.github.io/portfolio/ml/project-6/</guid><description>Text Representation Maps each word in the vocabulary(V) of the text corpus to a unique ID (integer value), then represent each sentence or document in the corpus as a V-dimensional vector.
One-Hot encoding: In one-hot encoding, each word w in the corpus vocabulary is given a unique integer ID w id that is between 1 and |V|, where V is the set of the corpus vocabulary. Each word is then represented by a V-dimensional binary vector of 0s and 1s.</description></item><item><title>Day 5- NER</title><link>https://shikshya1.github.io/portfolio/ml/project-5/</link><pubDate>Wed, 05 Oct 2022 10:58:08 -0400</pubDate><guid>https://shikshya1.github.io/portfolio/ml/project-5/</guid><description>Named Entity Recognition NER refers to the IE task of identifying the entities in a document. Entities are typically names of persons, locations, and organizations, and other specialized strings, such as money expressions, dates, products, names/numbers of laws or articles, and so on. NER is an important step in the pipeline of several NLP applications involving information extraction.
A simple approach to building an NER system is to maintain a large collection of person/organization/location names that are the most relevant to our company (e.</description></item><item><title>Day 4- Text Classification</title><link>https://shikshya1.github.io/portfolio/ml/project-4/</link><pubDate>Tue, 04 Oct 2022 10:58:08 -0400</pubDate><guid>https://shikshya1.github.io/portfolio/ml/project-4/</guid><description>Text Classification Classification categorizes data into one or more known classes. The data can be text, speech, image or numeric. Classification is used in wide range of applications across multiple domains such as: social media, healthcare, e-commerce etc. In today&amp;rsquo;s post, I will be exploring text classification.
This notebook linked with this post demostrates how to use several ML algorigthms for the disaster classification task. The models built for this purpose in this notebook are:</description></item><item><title>Day 3- t-SNE</title><link>https://shikshya1.github.io/portfolio/ml/project-3/</link><pubDate>Mon, 03 Oct 2022 10:58:08 -0400</pubDate><guid>https://shikshya1.github.io/portfolio/ml/project-3/</guid><description>t-Distributed Stochastic Neighbor Embedding (t-SNE) t-SNE is a unsupervised, non-parametric (non-linear) dimensionality reduction method used for visualization and exploration of high-dimensional datasets. It gives an idea of how data is arranged in higher dimension. It&amp;rsquo;s hard for us to visualize data beyond 3 dimension. Standard visualization methods can usually capture one or two variable at a time. In such cases, dimension reduction algorithm can help to analyze the pattern in the data.</description></item><item><title>Day 2- Interpreting Classification Models</title><link>https://shikshya1.github.io/portfolio/ml/project-2/</link><pubDate>Sun, 02 Oct 2022 10:58:08 -0400</pubDate><guid>https://shikshya1.github.io/portfolio/ml/project-2/</guid><description>Interpreting Classification Models When we are building classification models; we are most of the time concerned about the performance criteria (how accurate the model is to classify the output labels). We donot look for explanations as to why it decided to classify the input to a certain class. But there are situations where questioning the decision of classification model becomes necessary.
Suppose we are building classifier that identifies spam emails. It identifies certain email as &amp;lsquo;spam&amp;rsquo; and moves it to spam folder rather than placing it in inbox folder without the need of human intervention.</description></item><item><title>Day 1- Feature Scaling</title><link>https://shikshya1.github.io/portfolio/ml/project-1/</link><pubDate>Sat, 01 Oct 2022 10:58:08 -0400</pubDate><guid>https://shikshya1.github.io/portfolio/ml/project-1/</guid><description>Feature Scaling Feature scaling is one of the most important transformation in most of the ML projects. When one feature is on small range; say 0 to 10 while the other one is on a large range (suppose 0 to 10000); ML algorithms donot perform well. We have to scale the features so that both of them takes a comparable ranges of values to each other. In simpler terms, it means transforming data into a common range of values.</description></item></channel></rss>