<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>Feature Scaling | Shikshya Dahal</title><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="Feature Scaling Feature scaling is one of the most important transformation in most of the ML projects. When one feature is on small range; say 0 to 10 while the other one is on a large range (suppose 0 to 10000); ML algorithms donot perform well. We have to scale the features so that both of them takes a comparable ranges of values to each other. In simpler terms, it means transforming data into a common range of values."><meta name=generator content="Hugo 0.105.0"><meta name=robots content="noindex, nofollow"><link rel=stylesheet href=/portfolio/ananke/css/main.min.css><meta property="og:title" content="Feature Scaling"><meta property="og:description" content="Feature Scaling Feature scaling is one of the most important transformation in most of the ML projects. When one feature is on small range; say 0 to 10 while the other one is on a large range (suppose 0 to 10000); ML algorithms donot perform well. We have to scale the features so that both of them takes a comparable ranges of values to each other. In simpler terms, it means transforming data into a common range of values."><meta property="og:type" content="article"><meta property="og:url" content="https://shikshya1.github.io/portfolio/ml/project-1/"><meta property="article:section" content="ML"><meta property="article:published_time" content="2022-10-01T10:58:08-04:00"><meta property="article:modified_time" content="2022-10-01T10:58:08-04:00"><meta property="og:site_name" content="Shikshya Dahal"><meta itemprop=name content="Feature Scaling"><meta itemprop=description content="Feature Scaling Feature scaling is one of the most important transformation in most of the ML projects. When one feature is on small range; say 0 to 10 while the other one is on a large range (suppose 0 to 10000); ML algorithms donot perform well. We have to scale the features so that both of them takes a comparable ranges of values to each other. In simpler terms, it means transforming data into a common range of values."><meta itemprop=datePublished content="2022-10-01T10:58:08-04:00"><meta itemprop=dateModified content="2022-10-01T10:58:08-04:00"><meta itemprop=wordCount content="382"><meta itemprop=keywords content="Normalization,ML,"><meta name=twitter:card content="summary"><meta name=twitter:title content="Feature Scaling"><meta name=twitter:description content="Feature Scaling Feature scaling is one of the most important transformation in most of the ML projects. When one feature is on small range; say 0 to 10 while the other one is on a large range (suppose 0 to 10000); ML algorithms donot perform well. We have to scale the features so that both of them takes a comparable ranges of values to each other. In simpler terms, it means transforming data into a common range of values."></head><body class="ma0 avenir bg-near-white"><header class="cover bg-top" style=background-image:url(https://shikshya1.github.io/portfolio/images/cpv.jpg)><div class=bg-black-60><nav class="pv3 ph3 ph4-ns" role=navigation><div class="flex-l justify-between items-center center"><a href=/portfolio/ class="f3 fw2 hover-white no-underline white-90 dib">Shikshya Dahal</a><div class="flex-l items-center"><ul class="pl0 mr3"><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/portfolio/about/ title="About page">About</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/portfolio/contact/ title="Contact page">Contact</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/portfolio/ml/ title="ML Basics page">ML Basics</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/portfolio/nlp/ title="NLP page">NLP</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/portfolio/projects/ title="Projects page">Projects</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/portfolio/statistics/ title="Statistics page">Statistics</a></li></ul><div class=ananke-socials><a href=https://github.com/shikshya1 target=_blank class="github ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="GitHub link" rel=noopener aria-label="follow on GitHub——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 512 512" viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M256 32C132.3 32 32 134.8 32 261.7c0 101.5 64.2 187.5 153.2 217.9 11.2 2.1 15.3-5 15.3-11.1.0-5.5-.2-19.9-.3-39.1-62.3 13.9-75.5-30.8-75.5-30.8-10.2-26.5-24.9-33.6-24.9-33.6-20.3-14.3 1.5-14 1.5-14 22.5 1.6 34.3 23.7 34.3 23.7 20 35.1 52.4 25 65.2 19.1 2-14.8 7.8-25 14.2-30.7-49.7-5.8-102-25.5-102-113.5.0-25.1 8.7-45.6 23-61.6-2.3-5.8-10-29.2 2.2-60.8.0.0 18.8-6.2 61.6 23.5 17.9-5.1 37-7.6 56.1-7.7 19 .1 38.2 2.6 56.1 7.7 42.8-29.7 61.5-23.5 61.5-23.5 12.2 31.6 4.5 55 2.2 60.8 14.3 16.1 23 36.6 23 61.6.0 88.2-52.4 107.6-102.3 113.3 8 7.1 15.2 21.1 15.2 42.5.0 30.7-.3 55.5-.3 63 0 6.1 4 13.3 15.4 11C415.9 449.1 480 363.1 480 261.7 480 134.8 379.7 32 256 32z"/></svg></span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></span></a></div></div></div></nav><div class="tc-l pv6 ph3 ph4-ns"><h1 class="f2 f1-l fw2 white-90 mb0 lh-title">Feature Scaling</h1></div></div></header><main class=pb7 role=main><article class="flex-l flex-wrap justify-between mw8 center ph3"><header class="mt4 w-100"><aside class="instapaper_ignoref b helvetica tracked">ML BASICS</aside><div id=sharing class="mt3 ananke-socials"></div><h1 class="f1 athelas mt3 mb1">Feature Scaling</h1><time class="f6 mv4 dib tracked" datetime=2022-10-01T10:58:08-04:00>October 1, 2022</time></header><div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><h1 id=feature-scaling>Feature Scaling</h1><p>Feature scaling is one of the most important transformation in most of the ML projects. When one feature is on small range; say 0 to 10 while the other one is on a large range (suppose 0 to 10000); ML algorithms donot perform well. We have to scale the features so that both of them takes a comparable ranges of values to each other. In simpler terms, it means transforming data into a common range of values.</p><h4 id=when-should-we-use-feature-selection>When should we use feature selection?</h4><ol><li><p>While using distance-based metrics:
While using distance based methods like SVM, k-nearest neighbors; choosing not to scale data may drastically impact predictions leading to wrong interpretations.</p></li><li><p>While using regularization:
Penalty on weights depends heavily on the scale associated with the features. Applying regularization on two features with huge difference on scale will result in unfair punishment of feature with small range.</p></li></ol><p>Feature scaling can be done through:</p><ul><li>Normalization</li><li>Standardization</li></ul><h4 id=normalization>Normalization:</h4><p>Normalization also referred as Min-Max scaling is the simplest where values are shifted and rescaled such that data is scaled between 0 & 1.</p><p>X1 = (X-Xmin)/(Xmax - Xmin)</p><h4 id=standardization>Standardization:</h4><p>Standardization is done by subtracting each value from mean of column and dividing it by standard deviation. It centers the values around mean with a unit standard deviation. When the standardized data is plotted, it follows guassian distribution.</p><p>X1 = (X-μ)/σ</p><p>Standardization doesnot bound values to a specific range while normalization does which might be a problem for algorithms expecting values between 0 & 1. It is also robust to outliers.</p><h3 id=batch-normalization-in-deep-learning>Batch Normalization in Deep Learning</h3><ul><li>Batch Normalization:</li></ul><p>There are several ways in which we can initialize weights during training. But even that cannot ensure unstable gradient problem while training neural networks. Batch normalization addresses this issue; it zero centers and normalizes each input followed by scaling and shifting of result with two new parameters per layer (one for shifting and one for scaling). It helps to find good transformation to overcome the unstable gradient problem and helps in faster model training. Although the training epoch will take longer due to the added computation but it will converge faster. Also, there&rsquo;s no need to standardize the data before feeding into the neural network and reduces the need of using regularization techniques too.</p><h3 id=link-to-github-page-codehttpsgithubcomshikshya1ml_basicstreemainfeature20scaling>Link to github page: <a href=https://github.com/shikshya1/ML_Basics/tree/main/Feature%20Scaling>Code</a></h3><ul class=pa0><li class="list di"><a href=/tags/normalization class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">Normalization</a></li><li class="list di"><a href=/tags/ml class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">ML</a></li></ul><div class="mt6 instapaper_ignoref"></div></div><aside class="w-30-l mt6-l"></aside></article></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3"></a><div><div class=ananke-socials><a href=https://github.com/shikshya1 target=_blank class="github ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="GitHub link" rel=noopener aria-label="follow on GitHub——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 512 512" viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M256 32C132.3 32 32 134.8 32 261.7c0 101.5 64.2 187.5 153.2 217.9 11.2 2.1 15.3-5 15.3-11.1.0-5.5-.2-19.9-.3-39.1-62.3 13.9-75.5-30.8-75.5-30.8-10.2-26.5-24.9-33.6-24.9-33.6-20.3-14.3 1.5-14 1.5-14 22.5 1.6 34.3 23.7 34.3 23.7 20 35.1 52.4 25 65.2 19.1 2-14.8 7.8-25 14.2-30.7-49.7-5.8-102-25.5-102-113.5.0-25.1 8.7-45.6 23-61.6-2.3-5.8-10-29.2 2.2-60.8.0.0 18.8-6.2 61.6 23.5 17.9-5.1 37-7.6 56.1-7.7 19 .1 38.2 2.6 56.1 7.7 42.8-29.7 61.5-23.5 61.5-23.5 12.2 31.6 4.5 55 2.2 60.8 14.3 16.1 23 36.6 23 61.6.0 88.2-52.4 107.6-102.3 113.3 8 7.1 15.2 21.1 15.2 42.5.0 30.7-.3 55.5-.3 63 0 6.1 4 13.3 15.4 11C415.9 449.1 480 363.1 480 261.7 480 134.8 379.7 32 256 32z"/></svg></span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></span></a></div></div></div></footer></body></html>