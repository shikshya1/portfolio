<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>Day 6- Text Representation | Shikshya Dahal</title><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="Text Representation Maps each word in the vocabulary(V) of the text corpus to a unique ID (integer value), then represent each sentence or document in the corpus as a V-dimensional vector.
One-Hot encoding: In one-hot encoding, each word w in the corpus vocabulary is given a unique integer ID w id that is between 1 and |V|, where V is the set of the corpus vocabulary. Each word is then represented by a V-dimensional binary vector of 0s and 1s."><meta name=generator content="Hugo 0.105.0"><meta name=robots content="noindex, nofollow"><link rel=stylesheet href=/portfolio/ananke/css/main.min.css><meta property="og:title" content="Day 6- Text Representation"><meta property="og:description" content="Text Representation Maps each word in the vocabulary(V) of the text corpus to a unique ID (integer value), then represent each sentence or document in the corpus as a V-dimensional vector.
One-Hot encoding: In one-hot encoding, each word w in the corpus vocabulary is given a unique integer ID w id that is between 1 and |V|, where V is the set of the corpus vocabulary. Each word is then represented by a V-dimensional binary vector of 0s and 1s."><meta property="og:type" content="article"><meta property="og:url" content="https://shikshya1.github.io/portfolio/ml/project-6/"><meta property="article:section" content="ML"><meta property="article:published_time" content="2022-10-06T10:58:08-04:00"><meta property="article:modified_time" content="2022-10-06T10:58:08-04:00"><meta property="og:site_name" content="Shikshya Dahal"><meta itemprop=name content="Day 6- Text Representation"><meta itemprop=description content="Text Representation Maps each word in the vocabulary(V) of the text corpus to a unique ID (integer value), then represent each sentence or document in the corpus as a V-dimensional vector.
One-Hot encoding: In one-hot encoding, each word w in the corpus vocabulary is given a unique integer ID w id that is between 1 and |V|, where V is the set of the corpus vocabulary. Each word is then represented by a V-dimensional binary vector of 0s and 1s."><meta itemprop=datePublished content="2022-10-06T10:58:08-04:00"><meta itemprop=dateModified content="2022-10-06T10:58:08-04:00"><meta itemprop=wordCount content="991"><meta itemprop=keywords content="Word2Vec,"><meta name=twitter:card content="summary"><meta name=twitter:title content="Day 6- Text Representation"><meta name=twitter:description content="Text Representation Maps each word in the vocabulary(V) of the text corpus to a unique ID (integer value), then represent each sentence or document in the corpus as a V-dimensional vector.
One-Hot encoding: In one-hot encoding, each word w in the corpus vocabulary is given a unique integer ID w id that is between 1 and |V|, where V is the set of the corpus vocabulary. Each word is then represented by a V-dimensional binary vector of 0s and 1s."></head><body class="ma0 avenir bg-near-white"><header class="cover bg-top" style=background-image:url(https://shikshya1.github.io/portfolio/images/cpv.jpg)><div class=bg-black-60><nav class="pv3 ph3 ph4-ns" role=navigation><div class="flex-l justify-between items-center center"><a href=/portfolio/ class="f3 fw2 hover-white no-underline white-90 dib">Shikshya Dahal</a><div class="flex-l items-center"><ul class="pl0 mr3"><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/portfolio/about/ title="About page">About</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/portfolio/contact/ title="Contact page">Contact</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/portfolio/ml/ title="ML Basics page">ML Basics</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/portfolio/nlp/ title="NLP page">NLP</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/portfolio/statistics/ title="Statistics page">Statistics</a></li></ul><div class=ananke-socials><a href=https://github.com/shikshya1 target=_blank class="github ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="GitHub link" rel=noopener aria-label="follow on GitHub——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 512 512" viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M256 32C132.3 32 32 134.8 32 261.7c0 101.5 64.2 187.5 153.2 217.9 11.2 2.1 15.3-5 15.3-11.1.0-5.5-.2-19.9-.3-39.1-62.3 13.9-75.5-30.8-75.5-30.8-10.2-26.5-24.9-33.6-24.9-33.6-20.3-14.3 1.5-14 1.5-14 22.5 1.6 34.3 23.7 34.3 23.7 20 35.1 52.4 25 65.2 19.1 2-14.8 7.8-25 14.2-30.7-49.7-5.8-102-25.5-102-113.5.0-25.1 8.7-45.6 23-61.6-2.3-5.8-10-29.2 2.2-60.8.0.0 18.8-6.2 61.6 23.5 17.9-5.1 37-7.6 56.1-7.7 19 .1 38.2 2.6 56.1 7.7 42.8-29.7 61.5-23.5 61.5-23.5 12.2 31.6 4.5 55 2.2 60.8 14.3 16.1 23 36.6 23 61.6.0 88.2-52.4 107.6-102.3 113.3 8 7.1 15.2 21.1 15.2 42.5.0 30.7-.3 55.5-.3 63 0 6.1 4 13.3 15.4 11C415.9 449.1 480 363.1 480 261.7 480 134.8 379.7 32 256 32z"/></svg></span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></span></a></div></div></div></nav><div class="tc-l pv6 ph3 ph4-ns"><h1 class="f2 f1-l fw2 white-90 mb0 lh-title">Day 6- Text Representation</h1></div></div></header><main class=pb7 role=main><article class="flex-l flex-wrap justify-between mw8 center ph3"><header class="mt4 w-100"><aside class="instapaper_ignoref b helvetica tracked">ML BASICS</aside><div id=sharing class="mt3 ananke-socials"></div><h1 class="f1 athelas mt3 mb1">Day 6- Text Representation</h1><time class="f6 mv4 dib tracked" datetime=2022-10-06T10:58:08-04:00>October 6, 2022</time></header><div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><h1 id=text-representation>Text Representation</h1><p>Maps each word in the vocabulary(V) of the text corpus to a unique ID (integer value), then represent each sentence or document in the corpus as a V-dimensional vector.</p><h3 id=one-hot-encoding>One-Hot encoding:</h3><p>In one-hot encoding, each word w in the corpus vocabulary is given a unique integer ID w id that is between 1 and |V|, where V is the set of the corpus vocabulary. Each word is then represented by a V-dimensional binary vector of 0s and 1s.</p><h3 id=bag-of-words>Bag of Words:</h3><p>Represents the text under consideration as a bag (collection) of words while ignoring the order and context.</p><p>Advantages:</p><ul><li>Simple to understand and implement.</li><li>With this representation, documents having the same words will have their vector representations closer to each other in Euclidean space as compared to documents with completely different words.</li></ul><p>Disadvantages:</p><p>-The size of the vector increases with the size of the vocabulary. Thus, sparsity continues to be a problem. One way to control it is by limiting the vocabulary to n number of the most frequent words.</p><ul><li>It does not capture the similarity between different words that mean the same thing. Say we have three documents: “I run”, “I ran”, and “I ate”. BoW vectors of all three documents will be equally apart.</li><li>This representation does not have any way to handle out of vocabulary words (i.e., new words that were not seen in the corpus that was used to build the vectorizer).</li><li>As the name indicates, it is a “bag” of words—word order information is lost in this representation.</li></ul><h3 id=bag-of-n-grams>Bag of N-Grams:</h3><p>Breaks text into chunks of n contiguous words (or tokens). This can help us capture some context, which earlier approaches could not do. Each chunk is called an n-gram. The corpus vocabulary, V, is then nothing but a collection of all unique n-grams across the text corpus. Then, each document in the
corpus is represented by a vector of length |V|. This vector simply contains the frequency counts of n-grams present in the document and zero for the n-grams that are not present.</p><p>Pros and Cons:</p><ul><li>It captures some context and word-order information in the form of n-grams.</li><li>Thus, resulting vector space is able to capture some semantic similarity. Documents having the same n-grams will have their vectors closer to each other in Euclidean space as compared to documents with completely different n-grams.</li><li>As n increases, dimensionality (and therefore sparsity) only increases rapidly.</li><li>It still provides no way to address the OOV problem.</li></ul><h3 id=tf-idf>TF-IDF:</h3><p>The intuition behind TF-IDF is as follows: if a word w appears many times in a document di but does not occur much in the rest of the documents dj in the corpus, then the word w must be of great importance to the document d i . The importance of w should increase in proportion to its frequency in di , but at the same time, its importance should decrease in proportion to the word’s frequency in other documents dj in the corpus. Mathematically, this is captured using two quantities: TF and IDF. The two are then combined to arrive at the TF-IDF score.</p><p>Disadvantages :</p><p>• These are discrete representations- i.e: they treat language units (words, n-grams) as atomic units. This discreteness hampers the ability to capture relationship between words.
• The feature vectors are sparse and high-dimensional representations. The dimensionality increases with the size of the vocabulary, with most values being zero for any vector. This hampers learning capability. Further, high-dimensionality representation makes them computationally inefficient.
• They cannot handle OOV words.</p><h3 id=word-embeddings-word2vec>Word Embeddings (Word2Vec):</h3><p>Word2vec ensures that the learned word representations are low dimensional and dense (that is, most values in these vectors are non-zero). Such representations make ML tasks more tractable and efficient. Word2vec led to a lot of work (both pure and applied) in the direction of learning text representations using neural networks. These representations are also called “embeddings.”</p><p>Given a text corpus, the aim is to learn embeddings for every word in the corpus such that the word vector in the embedding space best captures the meaning of the word. To “derive” the meaning of the word, Word2vec uses distributional similarity and distributional hypothesis. That is, it derives the meaning of a word from its context: words that appear in its neighborhood in the text. So, if two different words (often) occur in similar context, then it’s highly likely that their meanings are also similar. Word2vec operationalizes this by projecting the meaning of the words in a vector space where words with similar meanings will tend to cluster together, and words with very different meanings are far from one another.</p><p>Architectural variants proposed in Word2Vec approach:</p><ol><li><p>Continuous bag of words (CBOW): In CBOW, the primary task is to build a language model that correctly predicts the center word given the context words in which the center word appears. What is a language model? It is a (statistical) model that tries to give a probability distribution over sequences of words. Given a sentence of, say, m words, it assigns a probability Pr(w 1 , w 2 , ….., w n ) to the whole sentence. The objective of a language
model is to assign probabilities in such a way that it gives high probability to “good” sentences and low probabilities to “bad” sentences. By good, we mean sentences that are semantically and syntactically correct. By bad, we mean sentences that are incorrect—semantically or syntactically or both. So, for a sentence like “The cat jumped over the dog,” it will try to assign a probability close to 1.0, whereas for a sentence like “jumped over the the cat dog,” it tries to assign a probability close to 0.0.</p></li><li><p>SkipGram: SkipGram is very similar to CBOW, with some minor changes. In SkipGram, the task is to predict the context words from the center word.</p></li></ol><h3 id=link-to-github-page-codehttpsgithubcomshikshya130_days_of_mltreemainday-620text20representation>Link to github page: <a href=https://github.com/shikshya1/30_days_of_ml/tree/main/Day-6%20(Text%20representation)>Code</a></h3><p>Note: The notebook contains sample implementation of one-hot encoding, TF-IDF and logistic regression with word2vec representation</p><ul class=pa0><li class="list di"><a href=/tags/word2vec class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">Word2Vec</a></li></ul><div class="mt6 instapaper_ignoref"></div></div><aside class="w-30-l mt6-l"></aside></article></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3"></a><div><div class=ananke-socials><a href=https://github.com/shikshya1 target=_blank class="github ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="GitHub link" rel=noopener aria-label="follow on GitHub——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 512 512" viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M256 32C132.3 32 32 134.8 32 261.7c0 101.5 64.2 187.5 153.2 217.9 11.2 2.1 15.3-5 15.3-11.1.0-5.5-.2-19.9-.3-39.1-62.3 13.9-75.5-30.8-75.5-30.8-10.2-26.5-24.9-33.6-24.9-33.6-20.3-14.3 1.5-14 1.5-14 22.5 1.6 34.3 23.7 34.3 23.7 20 35.1 52.4 25 65.2 19.1 2-14.8 7.8-25 14.2-30.7-49.7-5.8-102-25.5-102-113.5.0-25.1 8.7-45.6 23-61.6-2.3-5.8-10-29.2 2.2-60.8.0.0 18.8-6.2 61.6 23.5 17.9-5.1 37-7.6 56.1-7.7 19 .1 38.2 2.6 56.1 7.7 42.8-29.7 61.5-23.5 61.5-23.5 12.2 31.6 4.5 55 2.2 60.8 14.3 16.1 23 36.6 23 61.6.0 88.2-52.4 107.6-102.3 113.3 8 7.1 15.2 21.1 15.2 42.5.0 30.7-.3 55.5-.3 63 0 6.1 4 13.3 15.4 11C415.9 449.1 480 363.1 480 261.7 480 134.8 379.7 32 256 32z"/></svg></span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></span></a></div></div></div></footer></body></html>