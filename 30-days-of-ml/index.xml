<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>30 Days of ML on Shikshya Dahal</title><link>https://shikshya1.github.io/portfolio/30-days-of-ml/</link><description>Recent content in 30 Days of ML on Shikshya Dahal</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 02 Mar 2017 12:00:00 -0500</lastBuildDate><atom:link href="https://shikshya1.github.io/portfolio/30-days-of-ml/index.xml" rel="self" type="application/rss+xml"/><item><title>Day 4- Text Classification</title><link>https://shikshya1.github.io/portfolio/30-days-of-ml/project-4/</link><pubDate>Sun, 04 Sep 2022 10:58:08 -0400</pubDate><guid>https://shikshya1.github.io/portfolio/30-days-of-ml/project-4/</guid><description>Text Classification Classification categorizes data into one or more known classes. The data can be text, speech, image or numeric. Classification is used in wide range of applications across multiple domains such as: social media, healthcare, e-commerce etc. In today&amp;rsquo;s post, I will be exploring text classification.
This notebook linked with this post demostrates how to use several ML algorigthms for the disaster classification task. The models built for this purpose in this notebook are:</description></item><item><title>Day 3- t-SNE</title><link>https://shikshya1.github.io/portfolio/30-days-of-ml/project-3/</link><pubDate>Sat, 03 Sep 2022 10:58:08 -0400</pubDate><guid>https://shikshya1.github.io/portfolio/30-days-of-ml/project-3/</guid><description>t-Distributed Stochastic Neighbor Embedding (t-SNE) t-SNE is a unsupervised, non-parametric (non-linear) dimensionality reduction method used for visualization and exploration of high-dimensional datasets. It gives an idea of how data is arranged in higher dimension. It&amp;rsquo;s hard for us to visualize data beyond 3 dimension. Standard visualization methods can usually capture one or two variable at a time. In such cases, dimension reduction algorithm can help to analyze the pattern in the data.</description></item><item><title>Day 2- Interpreting Classification Models</title><link>https://shikshya1.github.io/portfolio/30-days-of-ml/project-2/</link><pubDate>Fri, 02 Sep 2022 10:58:08 -0400</pubDate><guid>https://shikshya1.github.io/portfolio/30-days-of-ml/project-2/</guid><description>Interpreting Classification Models When we are building classification models; we are most of the time concerned about the performance criteria (how accurate the model is to classify the output labels). We donot look for explanations as to why it decided to classify the input to a certain class. But there are situations where questioning the decision of classification model becomes necessary.
Suppose we are building classifier that identifies spam emails. It identifies certain email as &amp;lsquo;spam&amp;rsquo; and moves it to spam folder rather than placing it in inbox folder without the need of human intervention.</description></item><item><title>Day 1- Feature Scaling</title><link>https://shikshya1.github.io/portfolio/30-days-of-ml/project-1/</link><pubDate>Thu, 01 Sep 2022 10:58:08 -0400</pubDate><guid>https://shikshya1.github.io/portfolio/30-days-of-ml/project-1/</guid><description>Feature Scaling Feature scaling is one of the most important transformation in most of the ML projects. When one feature is on small range; say 0 to 10 while the other one is on a large range (suppose 0 to 10000); ML algorithms donot perform well. We have to scale the features so that both of them takes a comparable ranges of values to each other. In simpler terms, it means transforming data into a common range of values.</description></item></channel></rss>