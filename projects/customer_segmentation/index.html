<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>Customer segmentation of retail dataset using kmeans clustering algorithm | Shikshya Dahal</title><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="Customer Segmentation
customer segmentation is a process of dividing customers into different groups based on their characteristics. This can be useful for mall management in order to better understand their customers and tailor marketing strategies to different groups. One popular method for customer segmentation is k-means clustering, which is a machine learning technique that groups similar data points together. In this article, we will be discussing how to use the k-means algorithm in Python to perform customer segmentation for a mall."><meta name=generator content="Hugo 0.108.0"><meta name=robots content="noindex, nofollow"><link rel=stylesheet href=/portfolio/ananke/css/main.min.css><meta property="og:title" content="Customer segmentation of retail dataset using kmeans clustering algorithm"><meta property="og:description" content="Customer Segmentation
customer segmentation is a process of dividing customers into different groups based on their characteristics. This can be useful for mall management in order to better understand their customers and tailor marketing strategies to different groups. One popular method for customer segmentation is k-means clustering, which is a machine learning technique that groups similar data points together. In this article, we will be discussing how to use the k-means algorithm in Python to perform customer segmentation for a mall."><meta property="og:type" content="article"><meta property="og:url" content="https://shikshya1.github.io/portfolio/projects/customer_segmentation/"><meta property="article:section" content="Projects"><meta property="article:published_time" content="2022-11-05T10:58:08-04:00"><meta property="article:modified_time" content="2022-11-05T10:58:08-04:00"><meta property="og:site_name" content="Shikshya Dahal"><meta itemprop=name content="Customer segmentation of retail dataset using kmeans clustering algorithm"><meta itemprop=description content="Customer Segmentation
customer segmentation is a process of dividing customers into different groups based on their characteristics. This can be useful for mall management in order to better understand their customers and tailor marketing strategies to different groups. One popular method for customer segmentation is k-means clustering, which is a machine learning technique that groups similar data points together. In this article, we will be discussing how to use the k-means algorithm in Python to perform customer segmentation for a mall."><meta itemprop=datePublished content="2022-11-05T10:58:08-04:00"><meta itemprop=dateModified content="2022-11-05T10:58:08-04:00"><meta itemprop=wordCount content="1138"><meta itemprop=keywords content="k-means,clustering,"><meta name=twitter:card content="summary"><meta name=twitter:title content="Customer segmentation of retail dataset using kmeans clustering algorithm"><meta name=twitter:description content="Customer Segmentation
customer segmentation is a process of dividing customers into different groups based on their characteristics. This can be useful for mall management in order to better understand their customers and tailor marketing strategies to different groups. One popular method for customer segmentation is k-means clustering, which is a machine learning technique that groups similar data points together. In this article, we will be discussing how to use the k-means algorithm in Python to perform customer segmentation for a mall."></head><body class="ma0 avenir bg-near-white"><header class="cover bg-top" style=background-image:url(https://shikshya1.github.io/portfolio/images/cpv.jpg)><div class=bg-black-60><nav class="pv3 ph3 ph4-ns" role=navigation><div class="flex-l justify-between items-center center"><a href=/portfolio/ class="f3 fw2 hover-white no-underline white-90 dib">Shikshya Dahal</a><div class="flex-l items-center"><ul class="pl0 mr3"><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/portfolio/about/ title="About page">About</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/portfolio/aws/ title="AWS page">AWS</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/portfolio/ml/ title="ML Basics page">ML Basics</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/portfolio/nlp/ title="NLP page">NLP</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/portfolio/projects/ title="Projects page">Projects</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/portfolio/statistics/ title="Statistics page">Statistics</a></li></ul><div class=ananke-socials><a href=https://github.com/shikshya1 target=_blank class="github ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="GitHub link" rel=noopener aria-label="follow on GitHub——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 512 512" viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M256 32C132.3 32 32 134.8 32 261.7c0 101.5 64.2 187.5 153.2 217.9 11.2 2.1 15.3-5 15.3-11.1.0-5.5-.2-19.9-.3-39.1-62.3 13.9-75.5-30.8-75.5-30.8-10.2-26.5-24.9-33.6-24.9-33.6-20.3-14.3 1.5-14 1.5-14 22.5 1.6 34.3 23.7 34.3 23.7 20 35.1 52.4 25 65.2 19.1 2-14.8 7.8-25 14.2-30.7-49.7-5.8-102-25.5-102-113.5.0-25.1 8.7-45.6 23-61.6-2.3-5.8-10-29.2 2.2-60.8.0.0 18.8-6.2 61.6 23.5 17.9-5.1 37-7.6 56.1-7.7 19 .1 38.2 2.6 56.1 7.7 42.8-29.7 61.5-23.5 61.5-23.5 12.2 31.6 4.5 55 2.2 60.8 14.3 16.1 23 36.6 23 61.6.0 88.2-52.4 107.6-102.3 113.3 8 7.1 15.2 21.1 15.2 42.5.0 30.7-.3 55.5-.3 63 0 6.1 4 13.3 15.4 11C415.9 449.1 480 363.1 480 261.7 480 134.8 379.7 32 256 32z"/></svg></span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></span></a></div></div></div></nav><div class="tc-l pv6 ph3 ph4-ns"></div></div></header><main class=pb7 role=main><article class="flex-l flex-wrap justify-between mw8 center ph3"><header class="mt4 w-100"><aside class="instapaper_ignoref b helvetica tracked">PROJECTS</aside><div id=sharing class="mt3 ananke-socials"></div><h1 class="f1 athelas mt3 mb1">Customer segmentation of retail dataset using kmeans clustering algorithm</h1><time class="f6 mv4 dib tracked" datetime=2022-11-05T10:58:08-04:00>November 5, 2022</time></header><div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><p>Customer Segmentation</p><p>customer segmentation is a process of dividing customers into different groups based on their characteristics. This can be useful for mall management in order to better understand their customers and tailor marketing strategies to different groups. One popular method for customer segmentation is k-means clustering, which is a machine learning technique that groups similar data points together. In this article, we will be discussing how to use the k-means algorithm in Python to perform customer segmentation for a mall.</p><h3 id=k-means-algorithm>k-means Algorithm</h3><p>k-means algorithm is a simple yet powerful method for grouping similar data points together. It works by dividing a dataset into k clusters, where each cluster is represented by its centroid (mean). The algorithm iteratively assigns each data point to the cluster with the nearest centroid, and then updates the centroid based on the new data point assignments.</p><p>The first step is to import the necessary libraries, including scikit-learn, numpy, pandas, seaborn and matplotlib.</p><pre tabindex=0><code># Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from scipy import stats

import seaborn as sns
import matplotlib.pyplot as plt
</code></pre><p>The link for the dataset that I will be using in this article : <a href=https://www.kaggle.com/datasets/vjchoudhary7/customer-segmentation-tutorial-in-python>https://www.kaggle.com/datasets/vjchoudhary7/customer-segmentation-tutorial-in-python</a></p><pre tabindex=0><code>data = pd.read_csv(&#34;Mall_Customers.csv&#34;)
</code></pre><h4 id=data-cleaning>Data Cleaning:</h4><p>The first step in preprocessing is data cleaning, which involves removing or correcting any errors or inconsistencies in the dataset.</p><p>Check if there is missing or duplicate data</p><pre tabindex=0><code>print(data.isnull().sum())
# drop null values if present
data.duplicated().sum()
</code></pre><p>Renaming columns</p><pre tabindex=0><code>data.rename(columns = {&#39;Age&#39;:&#39;age&#39;, &#39;Annual Income (k$)&#39;: &#39;income&#39;, &#39;Spending Score (1-100)&#39;: &#39;spending&#39;}, inplace=True)
</code></pre><p>Dealing with outliers</p><pre tabindex=0><code># check for outliers using box plot
sns.boxplot(data=data[[&#39;age&#39;, &#39;income&#39;, &#39;spending&#39;]])
plt.show()
</code></pre><p>We notice outliers present in income. Outliers are values that are significantly different from the rest of the data in a dataset. These values can have a negative impact on the performance of a machine learning model, as they can skew the results. One of the most common ways to identify and remove outliers is through the use of the z-score.</p><p>The z-score is a statistical measure that is used to indicate how many standard deviations a data point is from the mean. It is calculated by subtracting the mean from the data point and dividing by the standard deviation. A z-score of 0 indicates that the data point is exactly the same as the mean, while a z-score of 1 indicates that the data point is one standard deviation above the mean.</p><p>The z-score is most commonly used to remove outliers when the data is approximately normally distributed. This is because the z-score is based on the normal distribution, which is a bell-shaped distribution that is often assumed for many types of data. When the data is normally distributed, outliers can be easily identified by their z-scores, as they will have a z-score that is significantly larger than the rest of the data.</p><p>It&rsquo;s worth mentioning that the z-score method will not work well with data that is not normally distributed. In that case, other methods such as the Interquartile range (IQR) method can be used.</p><p>we can set a threshold, usually 3 or -3, to identify and remove outliers. Any data point with a z-score above or below this threshold can be considered an outlier and can be removed from the dataset.</p><pre tabindex=0><code># remove outliers using z-score
data = data[(np.abs(stats.zscore(data[[ &#39;income&#39;]])) &lt; 3).all(axis=1)]
</code></pre><p>Since, customerID is unique across all rows and doesnot contribute to the accuracy of clustering algorithm. We will drop the column.</p><pre tabindex=0><code>data.drop(&#39;CustomerID&#39;, axis=1, inplace=True)
</code></pre><p>Pairwise relationships between variables is plotted with hue gender. We can observe the distribution of male and female across all features to be similar. So, Gender feature won&rsquo;t contribute much to the performance of clustering algorithm. Thus, we won&rsquo;t be using Gender column during clustering.</p><pre tabindex=0><code># Select features for clustering
X = data[[&#39;age&#39;, &#39;income&#39;, &#39;spending&#39;]]
</code></pre><p>Next, we will perform standardization to ensure that all the features have the same scale. Z-score normalization, also known as standardization, is a technique that is used to standardize the data by subtracting the mean and dividing by the standard deviation. This technique is useful when the data is normally distributed, and it helps to ensure that the data has a mean of 0 and a standard deviation of 1.</p><pre tabindex=0><code># Standardize the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
</code></pre><p>Choosing optimal number of clusters</p><p>Choosing the optimal number of clusters in k-means clustering is an important step that can greatly impact the performance of the model. The optimal number of clusters is the one that best balances the trade-off between the complexity of the model and the accuracy of the clustering.</p><p>The elbow method is a technique used to determine the optimal number of clusters in a k-means clustering algorithm. The idea behind the elbow method is to run the k-means clustering algorithm for different values of k and then plot the within-cluster sum of squares (WCSS) against the number of clusters. The optimal number of clusters is chosen as the value of k where the change in WCSS begins to level off, creating an &ldquo;elbow&rdquo; shape in the plot.</p><pre tabindex=0><code># Elbow method
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init=&#39;k-means++&#39;, max_iter=300, n_init=10, random_state=0)
    kmeans.fit(X_scaled)
    wcss.append(kmeans.inertia_)

fig, ax = plt.subplots(figsize=(12, 8))
sns.lineplot(x=list(range(1, 11)), y=wcss, ax=ax)
ax.set_title(&#39;Elbow Method&#39;)
ax.set_xlabel(&#39;Clusters&#39;)
ax.set_ylabel(&#39;WCSS&#39;)
</code></pre><p>Silhouette score method is based on the average similarity measure between all instances of a cluster and the instances of the next closest cluster. It ranges between -1 and 1. The higher the silhouette score, the better the clustering.</p><pre tabindex=0><code># Silhouette score method
from sklearn.metrics import silhouette_score

silhouette_scores = []

for n_cluster in range(2, 11):
    kmeans = KMeans(n_clusters=n_cluster).fit(X_scaled)
    label = kmeans.labels_
    sil_coeff = silhouette_score(X_scaled, label, metric=&#39;euclidean&#39;)
    silhouette_scores.append(sil_coeff)

# Plot the silhouette scores
plt.plot(range(2, 11), silhouette_scores)
plt.xlabel(&#34;Number of Clusters&#34;)
plt.ylabel(&#34;Silhouette Score&#34;)
plt.show()
</code></pre><p>Based on the above 2 methods, we can see that optimal number of clusters is 6.</p><pre tabindex=0><code># K-means clustering
kmeans = KMeans(n_clusters=6, init=&#39;k-means++&#39;)

# Fit the k means algorithm on scaled data
kmeans.fit(X_scaled)

# Assign the labels to each row
data[&#39;clusters&#39;] = kmeans.labels_
</code></pre><p>Analysis of Clusters:</p><p>Cluster 0: Low income, High spending score, young age
Cluster 1: medium income, medium spending score, relatively young age
Cluster 2: medium income, medium spending score and relatively higher age
Cluster 3: medium-high income, high spending score , medium age
Cluster 4: medium-high income, low spending score, age distributed across all groups
Cluster 5: low income, low spending score, age distributed across all groups</p><p>The major clusters to target is cluster 0 and cluster 3. Since cluster 3 has high income and spending score, we can target to converge this group more by expanding marketing strategies. Cluster 0 seems to have youngsters with low income but high spending score, promotional marketing campaign that are engaging and trendy could be effective.</p><h3 id=the-code-for-the-implementation-customer-segmentation-is-linked-herehttpsgithubcomshikshya1projectstreemainclustering>The code for the implementation customer segmentation is linked <a href=https://github.com/shikshya1/projects/tree/main/clustering>here</a></h3><ul class=pa0><li class="list di"><a href=/tags/k-means class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">k-means</a></li><li class="list di"><a href=/tags/clustering class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">clustering</a></li></ul><div class="mt6 instapaper_ignoref"></div></div><aside class="w-30-l mt6-l"></aside></article></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3"></a><div><div class=ananke-socials><a href=https://github.com/shikshya1 target=_blank class="github ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="GitHub link" rel=noopener aria-label="follow on GitHub——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 512 512" viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M256 32C132.3 32 32 134.8 32 261.7c0 101.5 64.2 187.5 153.2 217.9 11.2 2.1 15.3-5 15.3-11.1.0-5.5-.2-19.9-.3-39.1-62.3 13.9-75.5-30.8-75.5-30.8-10.2-26.5-24.9-33.6-24.9-33.6-20.3-14.3 1.5-14 1.5-14 22.5 1.6 34.3 23.7 34.3 23.7 20 35.1 52.4 25 65.2 19.1 2-14.8 7.8-25 14.2-30.7-49.7-5.8-102-25.5-102-113.5.0-25.1 8.7-45.6 23-61.6-2.3-5.8-10-29.2 2.2-60.8.0.0 18.8-6.2 61.6 23.5 17.9-5.1 37-7.6 56.1-7.7 19 .1 38.2 2.6 56.1 7.7 42.8-29.7 61.5-23.5 61.5-23.5 12.2 31.6 4.5 55 2.2 60.8 14.3 16.1 23 36.6 23 61.6.0 88.2-52.4 107.6-102.3 113.3 8 7.1 15.2 21.1 15.2 42.5.0 30.7-.3 55.5-.3 63 0 6.1 4 13.3 15.4 11C415.9 449.1 480 363.1 480 261.7 480 134.8 379.7 32 256 32z"/></svg></span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></span></a></div></div></div></footer></body></html>