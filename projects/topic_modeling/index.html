<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>Topic modeling | Shikshya Dahal</title><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="Topic modeling is a statistical modeling that can be used to discover hidden (latent) themes from the collection of documents. Having information about the problems and opinions about a certain product or services can be very important for businesses. But discovering themes in a large mine of online reviews or comments in the internet can be a expensive process. Thus, several algorithms like NMF, LDA, BERT can be employed for this purpose."><meta name=generator content="Hugo 0.108.0"><meta name=robots content="noindex, nofollow"><link rel=stylesheet href=/portfolio/ananke/css/main.min.css><meta property="og:title" content="Topic modeling"><meta property="og:description" content="Topic modeling is a statistical modeling that can be used to discover hidden (latent) themes from the collection of documents. Having information about the problems and opinions about a certain product or services can be very important for businesses. But discovering themes in a large mine of online reviews or comments in the internet can be a expensive process. Thus, several algorithms like NMF, LDA, BERT can be employed for this purpose."><meta property="og:type" content="article"><meta property="og:url" content="https://shikshya1.github.io/portfolio/projects/topic_modeling/"><meta property="article:section" content="Projects"><meta property="article:published_time" content="2022-11-05T10:58:08-04:00"><meta property="article:modified_time" content="2022-11-05T10:58:08-04:00"><meta property="og:site_name" content="Shikshya Dahal"><meta itemprop=name content="Topic modeling"><meta itemprop=description content="Topic modeling is a statistical modeling that can be used to discover hidden (latent) themes from the collection of documents. Having information about the problems and opinions about a certain product or services can be very important for businesses. But discovering themes in a large mine of online reviews or comments in the internet can be a expensive process. Thus, several algorithms like NMF, LDA, BERT can be employed for this purpose."><meta itemprop=datePublished content="2022-11-05T10:58:08-04:00"><meta itemprop=dateModified content="2022-11-05T10:58:08-04:00"><meta itemprop=wordCount content="513"><meta itemprop=keywords content="LDA,BERTopic,"><meta name=twitter:card content="summary"><meta name=twitter:title content="Topic modeling"><meta name=twitter:description content="Topic modeling is a statistical modeling that can be used to discover hidden (latent) themes from the collection of documents. Having information about the problems and opinions about a certain product or services can be very important for businesses. But discovering themes in a large mine of online reviews or comments in the internet can be a expensive process. Thus, several algorithms like NMF, LDA, BERT can be employed for this purpose."></head><body class="ma0 avenir bg-near-white"><header class="cover bg-top" style=background-image:url(https://shikshya1.github.io/portfolio/images/cpv.jpg)><div class=bg-black-60><nav class="pv3 ph3 ph4-ns" role=navigation><div class="flex-l justify-between items-center center"><a href=/portfolio/ class="f3 fw2 hover-white no-underline white-90 dib">Shikshya Dahal</a><div class="flex-l items-center"><ul class="pl0 mr3"><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/portfolio/about/ title="About page">About</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/portfolio/aws/ title="AWS page">AWS</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/portfolio/ml/ title="ML Basics page">ML Basics</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/portfolio/nlp/ title="NLP page">NLP</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/portfolio/projects/ title="Projects page">Projects</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/portfolio/statistics/ title="Statistics page">Statistics</a></li></ul><div class=ananke-socials><a href=https://github.com/shikshya1 target=_blank class="github ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="GitHub link" rel=noopener aria-label="follow on GitHub——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 512 512" viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M256 32C132.3 32 32 134.8 32 261.7c0 101.5 64.2 187.5 153.2 217.9 11.2 2.1 15.3-5 15.3-11.1.0-5.5-.2-19.9-.3-39.1-62.3 13.9-75.5-30.8-75.5-30.8-10.2-26.5-24.9-33.6-24.9-33.6-20.3-14.3 1.5-14 1.5-14 22.5 1.6 34.3 23.7 34.3 23.7 20 35.1 52.4 25 65.2 19.1 2-14.8 7.8-25 14.2-30.7-49.7-5.8-102-25.5-102-113.5.0-25.1 8.7-45.6 23-61.6-2.3-5.8-10-29.2 2.2-60.8.0.0 18.8-6.2 61.6 23.5 17.9-5.1 37-7.6 56.1-7.7 19 .1 38.2 2.6 56.1 7.7 42.8-29.7 61.5-23.5 61.5-23.5 12.2 31.6 4.5 55 2.2 60.8 14.3 16.1 23 36.6 23 61.6.0 88.2-52.4 107.6-102.3 113.3 8 7.1 15.2 21.1 15.2 42.5.0 30.7-.3 55.5-.3 63 0 6.1 4 13.3 15.4 11C415.9 449.1 480 363.1 480 261.7 480 134.8 379.7 32 256 32z"/></svg></span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></span></a></div></div></div></nav><div class="tc-l pv6 ph3 ph4-ns"><h1 class="f2 f1-l fw2 white-90 mb0 lh-title">Topic modeling</h1></div></div></header><main class=pb7 role=main><article class="flex-l flex-wrap justify-between mw8 center ph3"><header class="mt4 w-100"><aside class="instapaper_ignoref b helvetica tracked">PROJECTS</aside><div id=sharing class="mt3 ananke-socials"></div><h1 class="f1 athelas mt3 mb1">Topic modeling</h1><time class="f6 mv4 dib tracked" datetime=2022-11-05T10:58:08-04:00>November 5, 2022</time></header><div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><p>Topic modeling is a statistical modeling that can be used to discover hidden (latent) themes from the collection of documents. Having information about the problems and opinions about a certain product or services can be very important for businesses. But discovering themes in a large mine of online reviews or comments in the internet can be a expensive process. Thus, several algorithms like NMF, LDA, BERT can be employed for this purpose. In this project, I&rsquo;ll be discussing about LDA.</p><p>LDA extracts latent (hidden) topics from the corpus. It assumes that the corpus is the mix of distribution of topics and topics is the mix of distribution of words.</p><h2 id=dataset>Dataset:</h2><p>The dataset scraped from reddit under title &lsquo;Depression&rsquo; was used for this purpose. The link to scrape reddit post is available <a href=https://github.com/shikshya1/projects/tree/main/reddit-scraper>here</a></p><h2 id=data-preprocessing>Data Preprocessing:</h2><ol><li>Split the data into sentences and sentences into word. Lowercase the words and remove the words with length less than two characters.</li><li>Remove stopwords by looking at the default set given by NLTK. The stopword&rsquo;s list was extended by looking at the words distribution and removing the ones that didnot add much meaning to the analysis.</li><li>The digits were also removed from the dataset.</li><li>Words were lemmatized before passing on to the LDA model.</li><li>Bigrams are created.</li></ol><h2 id=text-representation>Text representation:</h2><p>Dictionary and the corpus are the input passed to the LDA model. Created a dictionary from the preprocessed text. Filter out the tokens that appear in less than 5 documents and more than 90 documents.Use gensim doc2bow function to create dictionary that contains information about how many times the word appears.</p><h2 id=building-lda>Building LDA:</h2><p>Params:</p><ol><li>Number of topic</li><li>alpha : document-topic density</li><li>eta: topic-word density</li></ol><h2 id=interpreting-topics>Interpreting topics:</h2><ol><li>Topic 1: Topic 1 mainly has words like disorder, death,shame, lonelineness, illness which can be categorized as loss.</li><li>Topic 2: Topic 2 mainly has discussion around mental health topic with words like depression, anxiety, trauma, help, adhd and so on.</li><li>Topic 3: Topic 3 talks about relationship with words like parent, mom, brother, sister, sibling, best friend , daughter, cousin and so on</li><li>Topic 4: The words in topic 4 can be categorized as daily struggles as it contains words like school,course, college, work, career, weight and so on</li><li>Topic 5: Topic 5 contains the most miscellaneous words out of other topics. Words under several themes like world, mind, life, family, friend, feel, reason and so on. It contains themes from mental health to realtionship.</li></ol><h2 id=bertopic>BERTopic</h2><p>BERTopic uses contextual embeddings that can capture the contextual nature of the text. The structure of BERTopic (embeddings, UMAP, HBDSCAN, c-TF-IDF) can be used accordingly to adapt to the current advancements being made in language models, clustering algorithms and dimensionality reduction algorithm.</p><p>BERTopic vs LDA:</p><p>In cases where computing sentence embeddings can be expensive, LDA is preferred over BERTopic. The interpretation of topic plays a very major role in topic modeling. So, the evaluation of which technique is better is subjective based on the use cases. Both techniques can be employed to create useful topic represntations.</p><p>The code for the implementation of LDA and BERTopic is linked <a href=https://github.com/shikshya1/projects/blob/main/topic-modeling/reddit_topic_modeling.ipynb>here</a></p><ul class=pa0><li class="list di"><a href=/tags/lda class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">LDA</a></li><li class="list di"><a href=/tags/bertopic class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">BERTopic</a></li></ul><div class="mt6 instapaper_ignoref"></div></div><aside class="w-30-l mt6-l"></aside></article></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3"></a><div><div class=ananke-socials><a href=https://github.com/shikshya1 target=_blank class="github ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="GitHub link" rel=noopener aria-label="follow on GitHub——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 512 512" viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M256 32C132.3 32 32 134.8 32 261.7c0 101.5 64.2 187.5 153.2 217.9 11.2 2.1 15.3-5 15.3-11.1.0-5.5-.2-19.9-.3-39.1-62.3 13.9-75.5-30.8-75.5-30.8-10.2-26.5-24.9-33.6-24.9-33.6-20.3-14.3 1.5-14 1.5-14 22.5 1.6 34.3 23.7 34.3 23.7 20 35.1 52.4 25 65.2 19.1 2-14.8 7.8-25 14.2-30.7-49.7-5.8-102-25.5-102-113.5.0-25.1 8.7-45.6 23-61.6-2.3-5.8-10-29.2 2.2-60.8.0.0 18.8-6.2 61.6 23.5 17.9-5.1 37-7.6 56.1-7.7 19 .1 38.2 2.6 56.1 7.7 42.8-29.7 61.5-23.5 61.5-23.5 12.2 31.6 4.5 55 2.2 60.8 14.3 16.1 23 36.6 23 61.6.0 88.2-52.4 107.6-102.3 113.3 8 7.1 15.2 21.1 15.2 42.5.0 30.7-.3 55.5-.3 63 0 6.1 4 13.3 15.4 11C415.9 449.1 480 363.1 480 261.7 480 134.8 379.7 32 256 32z"/></svg></span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></span></a></div></div></div></footer></body></html>