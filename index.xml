<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Hello ðŸŒ¼ on Shikshya Dahal</title><link>https://shikshya1.github.io/portfolio/</link><description>Recent content in Hello ðŸŒ¼ on Shikshya Dahal</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sun, 04 Sep 2022 10:58:08 -0400</lastBuildDate><atom:link href="https://shikshya1.github.io/portfolio/index.xml" rel="self" type="application/rss+xml"/><item><title>Day 4- Text Classification</title><link>https://shikshya1.github.io/portfolio/30-days-of-ml/project-4/</link><pubDate>Sun, 04 Sep 2022 10:58:08 -0400</pubDate><guid>https://shikshya1.github.io/portfolio/30-days-of-ml/project-4/</guid><description>Text Classification Classification categorizes data into one or more known classes. The data can be text, speech, image or numeric. Classification is used in wide range of applications across multiple domains such as: social media, healthcare, e-commerce etc. In today&amp;rsquo;s post, I will be exploring text classification.
This notebook linked with this post demostrates how to use several ML algorigthms for the disaster classification task. The models built for this purpose in this notebook are:</description></item><item><title>Day 3- t-SNE</title><link>https://shikshya1.github.io/portfolio/30-days-of-ml/project-3/</link><pubDate>Sat, 03 Sep 2022 10:58:08 -0400</pubDate><guid>https://shikshya1.github.io/portfolio/30-days-of-ml/project-3/</guid><description>t-Distributed Stochastic Neighbor Embedding (t-SNE) t-SNE is a unsupervised, non-parametric (non-linear) dimensionality reduction method used for visualization and exploration of high-dimensional datasets. It gives an idea of how data is arranged in higher dimension. It&amp;rsquo;s hard for us to visualize data beyond 3 dimension. Standard visualization methods can usually capture one or two variable at a time. In such cases, dimension reduction algorithm can help to analyze the pattern in the data.</description></item><item><title>Day 2- Interpreting Classification Models</title><link>https://shikshya1.github.io/portfolio/30-days-of-ml/project-2/</link><pubDate>Fri, 02 Sep 2022 10:58:08 -0400</pubDate><guid>https://shikshya1.github.io/portfolio/30-days-of-ml/project-2/</guid><description>Interpreting Classification Models When we are building classification models; we are most of the time concerned about the performance criteria (how accurate the model is to classify the output labels). We donot look for explanations as to why it decided to classify the input to a certain class. But there are situations where questioning the decision of classification model becomes necessary.
Suppose we are building classifier that identifies spam emails. It identifies certain email as &amp;lsquo;spam&amp;rsquo; and moves it to spam folder rather than placing it in inbox folder without the need of human intervention.</description></item><item><title>Day 1- Feature Scaling</title><link>https://shikshya1.github.io/portfolio/30-days-of-ml/project-1/</link><pubDate>Thu, 01 Sep 2022 10:58:08 -0400</pubDate><guid>https://shikshya1.github.io/portfolio/30-days-of-ml/project-1/</guid><description>Feature Scaling Feature scaling is one of the most important transformation in most of the ML projects. When one feature is on small range; say 0 to 10 while the other one is on a large range (suppose 0 to 10000); ML algorithms donot perform well. We have to scale the features so that both of them takes a comparable ranges of values to each other. In simpler terms, it means transforming data into a common range of values.</description></item><item><title>NLP 1: NLP pipelines</title><link>https://shikshya1.github.io/portfolio/nlp/project-1/</link><pubDate>Tue, 07 Apr 2020 10:58:08 -0400</pubDate><guid>https://shikshya1.github.io/portfolio/nlp/project-1/</guid><description>NLP :
&amp;ldquo;As we all know, dealing with natural language is hard. It is hard from the standpoint of the child, who must spend many years acquiring a language (compare this time span to that required for the acquisition of motor skills such as eating solids, walking, or swimming), it is hard for the adult language learner, it is hard for the scientist who attempts to model the relevant phenomena, and it is hard for the engineer who attempts to build systems that deal with natural language input or output.</description></item><item><title>About</title><link>https://shikshya1.github.io/portfolio/about/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://shikshya1.github.io/portfolio/about/</guid><description>Hi. My Name is Shikshya Dahal. I am an Information Management Graduate with specialization in Data Science and AI. Currently, I&amp;rsquo;m working as a Machine Learning Developer and most of my works are in NLP and Conversational AI. Data science has always been my passion. I am good at handling data and I have a solid foundation and experience in building ML and statistical models. I find exploring and visualizing insights hidden in data to develop data driven solutions that aids in decision making more than admirable.</description></item><item><title>Contact</title><link>https://shikshya1.github.io/portfolio/contact/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://shikshya1.github.io/portfolio/contact/</guid><description>Reach out to me on these platforms:
platform URL Github: https://github.com/shikshya1 Gmail: shikshyadahal@gmail.com Linkedin: https://www.linkedin.com/in/shikshya-dahal/</description></item></channel></rss>